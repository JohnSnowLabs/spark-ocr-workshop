{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e842662e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "\n",
    "labels = [\"advertisement\",\n",
    "          \"budget\",\n",
    "          \"email\",\n",
    "          \"file_folder\",\n",
    "          \"form\",\n",
    "          \"handwritten\",\n",
    "          \"invoice\",\n",
    "          \"letter\",\n",
    "          \"memo\",\n",
    "          \"news_article\",\n",
    "          \"presentation\",\n",
    "          \"questionnaire\",\n",
    "          \"resume\",\n",
    "          \"scientific_publication\",\n",
    "          \"scientific_report\",\n",
    "          \"specification\"]\n",
    "labels.sort()\n",
    "id2label = {v: k for v, k in enumerate(labels)}\n",
    "label2id = {k: v for v, k in enumerate(labels)}\n",
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeac8b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from glob import glob\n",
    "import random\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "import torch\n",
    "from transformers import AdamW\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "files = sorted(glob('processed_data/part*'))\n",
    "print(\"total num of files\", len(files))\n",
    "random.shuffle(files)\n",
    "\n",
    "N = 510\n",
    "CHUNK_SIZE = 50\n",
    "CHUNKS_NUM = int(len(files) / CHUNK_SIZE) + 1\n",
    "print(\"num of chunks\", CHUNKS_NUM)\n",
    "\n",
    "def get_chunk(index, files_):\n",
    "    i = CHUNK_SIZE * index\n",
    "    df_tmp = pd.read_parquet(files_[i])\n",
    "    for f in files_[i + 1:i + CHUNK_SIZE]:\n",
    "        df_tmp = pd.concat([df_tmp,pd.read_parquet(f)])\n",
    "    return df_tmp\n",
    "\n",
    "def pad_or_cut(arr, n, padd_val = 0):\n",
    "    if len(arr) >= n:\n",
    "        return arr[:n]\n",
    "    else:\n",
    "        return np.pad(arr, (0,(n - len(arr))), 'constant',constant_values = padd_val)\n",
    "\n",
    "def preprocess_df(df, label_column = 'label'):\n",
    "    df['labels'] = df[label_column].apply(lambda x: label2id[x])\n",
    "    df['image_path'] = df['path']\n",
    "    df = df.drop(['path'], axis=1)\n",
    "    df_ = df.reset_index()\n",
    "    df_ = df_.drop([label_column], axis=1)\n",
    "    df_ = df_[df_['input_ids'].apply(lambda x: len(x) > 0)]\n",
    "    df_ = df_[df_['bbox'].apply(lambda x: len(x) > 0)]\n",
    "    df_ = df_[df_['attention_mask'].apply(lambda x: len(x) > 0)]\n",
    "    df_['input_ids'] = df_['input_ids'].apply(lambda x: pad_or_cut(np.array(x), N))\n",
    "    df_['attention_mask'] = df_['attention_mask'].apply(lambda x: pad_or_cut(np.array(x), N))\n",
    "    df_['bbox'] = df_['bbox'].apply(lambda x: np.reshape(pad_or_cut(x, N*4), (-1,4)))\n",
    "    return df_\n",
    "\n",
    "class PandasDataset(Dataset):\n",
    "    def __init__(self, dataframe, target_key, device):\n",
    "        self.dataframe = dataframe\n",
    "        self.target_key = target_key\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe[self.target_key])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return {k: torch.tensor(self.dataframe[k][index]).to(self.device) for k in self.dataframe}\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(PandasDataset(df_[\n",
    "    ['image_path', 'input_ids', 'attention_mask', 'bbox', 'labels']\n",
    "    ], 'labels', device), batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af037ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, LayoutLMv3FeatureExtractor, LayoutLMv3Processor\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"SCUT-DLVCLab/lilt-roberta-en-base\")\n",
    "feature_extractor = LayoutLMv3FeatureExtractor()\n",
    "processor = LayoutLMv3Processor(feature_extractor, tokenizer, apply_ocr = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f778cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "chunk_idxs = range(0, CHUNKS_NUM)\n",
    "trainingSet_idxs, testSet_idxs = train_test_split(chunk_idxs, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332bb57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"SCUT-DLVCLab/lilt-roberta-en-base\",\n",
    "                                                          num_labels=len(labels))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c15ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = 'fine_tuned_model'\n",
    "!mkdir $model_save_path\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "global_step = 0\n",
    "num_train_epochs = 5\n",
    "\n",
    "#put the model in training mode\n",
    "model.train()\n",
    "n = 0.0\n",
    "tot_n = len(trainingSet_idxs)\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "    print(\"Epoch:\", epoch)\n",
    "    for idx in trainingSet_idxs:\n",
    "        n = n + 1\n",
    "        print(\"Epoch:\", epoch, (n / tot_n))\n",
    "        df = get_chunk(idx, files)\n",
    "        dataloader = torch.utils.data.DataLoader(PandasDataset(preprocess_df(df, 'act_label')[\n",
    "            ['input_ids', 'attention_mask', 'bbox', 'labels']    \n",
    "            ], 'labels', device), batch_size=8)\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        try:\n",
    "            for batch in tqdm(dataloader):\n",
    "                # forward pass\n",
    "                outputs = model(**batch)\n",
    "                loss = outputs.loss\n",
    "                running_loss += loss.item()\n",
    "                predictions = outputs.logits.argmax(-1)\n",
    "                correct += (predictions == batch['labels']).float().sum()\n",
    "                # backward pass to get the gradients \n",
    "                loss.backward()\n",
    "                # update\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                global_step += 1\n",
    "        except:\n",
    "            print(\"Exception\")\n",
    "    model.save_pretrained(model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256a617f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_save_path)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f266759d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "correct = 0\n",
    "tot = 0\n",
    "N = 510\n",
    "for idx in testSet_idxs:\n",
    "    df = get_chunk(idx, files)\n",
    "    tot = tot + len(df)\n",
    "    dataloader = torch.utils.data.DataLoader(PandasDataset(preprocess_df(df, \"act_label\")[\n",
    "        ['input_ids', 'attention_mask', 'bbox', 'labels']\n",
    "        ], 'labels', device), batch_size=1)\n",
    "    try:\n",
    "        for batch in tqdm(dataloader):\n",
    "            outputs = model(**batch)\n",
    "            predictions = outputs.logits.argmax(-1)\n",
    "            correct += (predictions == batch['labels']).float().sum()\n",
    "    except:\n",
    "        accuracy = 100 * correct / tot\n",
    "        print(\"Testing accuracy:\", accuracy.item()) \n",
    "accuracy = 100 * correct / tot\n",
    "print(\"Testing accuracy:\", accuracy.item()) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
