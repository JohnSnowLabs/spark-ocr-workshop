{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yne_s9lKIqyw"
   },
   "source": [
    "#1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "zq31wV3CVpPu"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%cd /content\n",
    "\n",
    "%pip install transformers\n",
    "%pip install tensorflow\n",
    "\n",
    "!test -d unilm || git clone https://github.com/arvisioncode/unilm-dit.git unilm/\n",
    "%cd unilm/dit\n",
    "%pip install -r requirements.txt\n",
    "!pip3 install opencv-python\n",
    "!git clone https://github.com/NVIDIA/apex\n",
    "%cd apex\n",
    "!pip install shapely\n",
    "!pip install -v --disable-pip-version-check --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\"\n",
    "%cd /content\n",
    "\n",
    "!python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'\n",
    "!pip install shapely\n",
    "!pip install anyconfig\n",
    "!pip install munch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3aU1DkHYVtSh",
    "outputId": "ddff4e43-238a-498c-f42f-5e23de0df542"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1+cu117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-03 13:21:55.197407: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-03 13:21:56.184667: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.13.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "import tensorflow\n",
    "print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "_fv4EogMV-41"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install torch torchvision\n",
    "!pip install onnx onnxruntime onnxruntime-tools\n",
    "!pip install webdataset\n",
    "!pip install deepspeed==0.4.0\n",
    "!pip install timm==0.5.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_eu_MfQM65Pf"
   },
   "source": [
    "##1.2. Download Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PaBxlmPifKLd"
   },
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5rVviwkrChiZ",
    "outputId": "c9b06b67-fca3-40e8-fc01-393368e35125"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘models’: File exists\n",
      "--2023-08-03 13:30:31--  https://layoutlm.blob.core.windows.net/dit/dit-pts/dit-base-224-p16-500k-62d53a.pth?sv=2022-11-02&ss=b&srt=o&sp=r&se=2033-06-08T16:48:15Z&st=2023-06-08T08:48:15Z&spr=https&sig=a9VXrihTzbWyVfaIDlIT1Z0FoR1073VB0RLQUMuudD4%3D\n",
      "Resolving layoutlm.blob.core.windows.net (layoutlm.blob.core.windows.net)... 20.150.78.68\n",
      "Connecting to layoutlm.blob.core.windows.net (layoutlm.blob.core.windows.net)|20.150.78.68|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1105492382 (1,0G) [application/zip]\n",
      "Saving to: ‘dit-base-224-p16-500k-62d53a.pth?sv=2022-11-02&ss=b&srt=o&sp=r&se=2033-06-08T16:48:15Z&st=2023-06-08T08:48:15Z&spr=https&sig=a9VXrihTzbWyVfaIDlIT1Z0FoR1073VB0RLQUMuudD4=’\n",
      "\n",
      "-02&ss=b&srt=o&sp=r   0%[                    ]   9,59M  65,9KB/s    eta 1h 56m ^C\n"
     ]
    }
   ],
   "source": [
    "MODEL_DOWNLOAD_LINK = \"https://layoutlm.blob.core.windows.net/dit/dit-pts/dit-base-224-p16-500k-62d53a.pth?sv=2022-11-02&ss=b&srt=o&sp=r&se=2033-06-08T16:48:15Z&st=2023-06-08T08:48:15Z&spr=https&sig=a9VXrihTzbWyVfaIDlIT1Z0FoR1073VB0RLQUMuudD4%3D\"\n",
    "!mkdir models\n",
    "!wget \"$MODEL_DOWNLOAD_LINK\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gFcxIYmhfLPz"
   },
   "source": [
    "Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "zDHdHhh5btp2"
   },
   "outputs": [],
   "source": [
    "dataset_path = \"rvlcdip_test_small/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ifv0OmKgVhW4"
   },
   "source": [
    "##2.3 Prepare dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tvxfgxf9nkds"
   },
   "source": [
    "*   Recives a classifier datset orthered by folders, without labels.\n",
    "\n",
    "*   Generates a dataset in the DiT FT format, generating the labels of these images in tran val and test.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CozKWDm_Vjq9"
   },
   "source": [
    "Format expected if you select --data_path \"/content/rvlcdip_test/\", later in the training step\n",
    "\n",
    "\n",
    "![image.png](data:image/png;base64,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)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8NvLJByyWuDS"
   },
   "source": [
    "Create labels from images paths and the class of the folder. This example is for rvlcdip dataset:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "c21A2zgjaQ2X"
   },
   "outputs": [],
   "source": [
    "class_dict = {\n",
    "    \"letter\": 0,\n",
    "    \"form\": 1,\n",
    "    \"email\": 2,\n",
    "    \"handwritten\": 3,\n",
    "    \"advertisement\": 4,\n",
    "    \"scientific_report\": 5,\n",
    "    \"scientific_publication\": 6,\n",
    "    \"specification\": 7,\n",
    "    \"file_folder\": 8,\n",
    "    \"news_article\": 9,\n",
    "    \"budget\": 10,\n",
    "    \"invoice\": 11,\n",
    "    \"presentation\": 12,\n",
    "    \"questionnaire\": 13,\n",
    "    \"resume\": 14,\n",
    "    \"memo\": 15\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dict = {\n",
    "    \"advertisement\": 0,\n",
    "    \"budget\": 1,\n",
    "    \"email\": 2,\n",
    "    \"file_filder\": 3,\n",
    "    \"form\": 4,\n",
    "    \"handwritten\": 5,\n",
    "    \"invoice\": 6,\n",
    "    \"letter\": 7,\n",
    "    \"memo\": 8,\n",
    "    \"news_article\": 9,\n",
    "    \"presentation\": 10,\n",
    "    \"questionnaire\": 11,\n",
    "    \"resume\": 12,\n",
    "    \"scientific_publication\": 13,\n",
    "    \"scientific_report\": 14,\n",
    "    \"specification\": 15\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YUlQ2MAUWUfN",
    "outputId": "74118ddb-a16c-4300-af25-430dcee6a446"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mv: cannot move 'rvlcdip_test_small/images' to a subdirectory of itself, 'rvlcdip_test_small/images/images'\n"
     ]
    }
   ],
   "source": [
    "!mkdir {dataset_path}images\n",
    "!mv {dataset_path}* {dataset_path}images\n",
    "!mkdir {dataset_path}labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "n2oj4rHJWzk6"
   },
   "outputs": [],
   "source": [
    "# Split data\n",
    "train_percentage = 0.8\n",
    "val_percentage = 0.1\n",
    "test_percentage = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hrSL8HLAl7ZS"
   },
   "outputs": [],
   "source": [
    "# ## SMALL DATASET\n",
    "# !rm -r {dataset_path}images/file_folder\n",
    "# !rm -r {dataset_path}images/form\n",
    "# !rm -r {dataset_path}images/handwritten\n",
    "# !rm -r {dataset_path}images/invoice\n",
    "# !rm -r {dataset_path}images/letter\n",
    "# !rm -r {dataset_path}images/memo\n",
    "# !rm -r {dataset_path}images/news_article\n",
    "# !rm -r {dataset_path}images/presentation\n",
    "# !rm -r {dataset_path}images/questionnaire\n",
    "# !rm -r {dataset_path}images/resume\n",
    "# !rm -r {dataset_path}images/scientific_publication\n",
    "# !rm -r {dataset_path}images/scientific_report\n",
    "# !rm -r {dataset_path}images/specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iAQY2uT6WmOn",
    "outputId": "eaeb5cba-5c7c-46d7-819e-5747560e38b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET SPLIT: \n",
      "   train samples:  1064\n",
      "   validation samples:  133\n",
      "   test samples:  133\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "data = []\n",
    "labels_path = dataset_path + \"labels/\"\n",
    "data_path = dataset_path + \"images/\"\n",
    "\n",
    "\n",
    "for class_folder in os.listdir(data_path):\n",
    "    class_path = os.path.join(data_path, class_folder)\n",
    "    class_name = class_path.split(\"/\")[-1]\n",
    "    # print(class_name)\n",
    "\n",
    "    for image_file in os.listdir(class_path):\n",
    "        image_path = os.path.join(class_folder, image_file)\n",
    "        # print(image_path)\n",
    "        data.append((image_path, class_name))\n",
    "\n",
    "# print(data)\n",
    "random.shuffle(data)\n",
    "\n",
    "num_samples = len(data)\n",
    "num_train = int(num_samples * train_percentage)\n",
    "num_val = int(num_samples * val_percentage)\n",
    "num_test = num_samples - num_train - num_val\n",
    "print(\"DATASET SPLIT: \")\n",
    "print(\"   train samples: \", num_train)\n",
    "print(\"   validation samples: \", num_val)\n",
    "print(\"   test samples: \", num_test)\n",
    "\n",
    "train_data = data[:num_train]\n",
    "val_data = data[num_train:num_train + num_val]\n",
    "test_data = data[num_train + num_val:]\n",
    "\n",
    "def save_data(data, filename):\n",
    "    with open(filename, \"w\") as file:\n",
    "        for image_path, class_label in data:\n",
    "            file.write(f\"{image_path} {str(class_dict[class_label])}\\n\")\n",
    "\n",
    "save_data(train_data, labels_path + \"train.txt\")\n",
    "save_data(val_data, labels_path + \"val.txt\")\n",
    "save_data(test_data, labels_path + \"test.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-f4jJrgseFW7"
   },
   "source": [
    "#2. DIT Classifier Fine Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "luG8nOqiEoEe"
   },
   "source": [
    "##2.1. Repository execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n-3t9DFkdHHe",
    "outputId": "252b46d8-f694-4ffa-c775-5b9a7ad1f062"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ayman/Downloads/projects/di_ft/unilm/dit/classification\n",
      "2023-08-07 21:37:10.896250: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-07 21:37:11.839495: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Not using distributed mode\n",
      "Namespace(batch_size=32, epochs=50, update_freq=2, save_ckpt_freq=10, eval_freq=10, model='beit_base_patch16_224', rel_pos_bias=False, abs_pos_emb=True, qkv_bias=False, layer_scale_init_value=1e-05, input_size=224, drop=0.0, attn_drop_rate=0.0, drop_path=0.2, disable_eval_during_finetuning=False, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=1.0, momentum=0.9, weight_decay=0.05, weight_decay_end=None, lr=0.0005, layer_decay=0.75, warmup_lr=1e-06, min_lr=1e-06, warmup_epochs=20, warmup_steps=-1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', crop_pct=None, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0, cutmix=0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='models/rvlcdip_dit-b.pth', model_key='model|module', model_prefix='', init_scale=0.001, use_mean_pooling=True, disable_weight_decay_on_rel_pos_bias=False, data_path='rvlcdip_test_small/', eval_data_path='rvlcdip_test_small/', nb_classes=16, imagenet_default_mean_and_std=False, data_set='rvlcdip', output_dir='output/dit-base-rvlcdip-test/', log_dir='output/dit-base-rvlcdip-test/tf', device='cuda', seed=0, resume='', auto_resume=True, save_ckpt=True, start_epoch=0, eval=False, dist_eval=False, num_workers=0, pin_mem=True, world_size=1, local_rank=-1, dist_on_itp=False, dist_url='env://', enable_deepspeed=False, zero_stage=0, distributed=False)\n",
      "Transform = \n",
      "RandomResizedCropAndInterpolation(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bicubic)\n",
      "RandomHorizontalFlip(p=0.5)\n",
      "RandAugment(n=2, ops=\n",
      "\tAugmentOp(name=AutoContrast, p=0.5, m=9, mstd=0.5)\n",
      "\tAugmentOp(name=Equalize, p=0.5, m=9, mstd=0.5)\n",
      "\tAugmentOp(name=Invert, p=0.5, m=9, mstd=0.5)\n",
      "\tAugmentOp(name=Rotate, p=0.5, m=9, mstd=0.5)\n",
      "\tAugmentOp(name=PosterizeIncreasing, p=0.5, m=9, mstd=0.5)\n",
      "\tAugmentOp(name=SolarizeIncreasing, p=0.5, m=9, mstd=0.5)\n",
      "\tAugmentOp(name=SolarizeAdd, p=0.5, m=9, mstd=0.5)\n",
      "\tAugmentOp(name=ColorIncreasing, p=0.5, m=9, mstd=0.5)\n",
      "\tAugmentOp(name=ContrastIncreasing, p=0.5, m=9, mstd=0.5)\n",
      "\tAugmentOp(name=BrightnessIncreasing, p=0.5, m=9, mstd=0.5)\n",
      "\tAugmentOp(name=SharpnessIncreasing, p=0.5, m=9, mstd=0.5)\n",
      "\tAugmentOp(name=ShearX, p=0.5, m=9, mstd=0.5)\n",
      "\tAugmentOp(name=ShearY, p=0.5, m=9, mstd=0.5)\n",
      "\tAugmentOp(name=TranslateXRel, p=0.5, m=9, mstd=0.5)\n",
      "\tAugmentOp(name=TranslateYRel, p=0.5, m=9, mstd=0.5))\n",
      "ToTensor()\n",
      "Normalize(mean=tensor([0.5000, 0.5000, 0.5000]), std=tensor([0.5000, 0.5000, 0.5000]))\n",
      "RandomErasing(p=0.25, mode=pixel, count=(1, 1))\n",
      "---------------------------\n",
      "////////////////////////////////////////\n",
      "resume/10117306_10117308.tif\n",
      "Number of the class = 16\n",
      "Transform = \n",
      "Resize(size=256, interpolation=bicubic, max_size=None, antialias=warn)\n",
      "CenterCrop(size=(224, 224))\n",
      "ToTensor()\n",
      "Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
      "---------------------------\n",
      "////////////////////////////////////////\n",
      "news_article/0000055301.tif\n",
      "Number of the class = 16\n",
      "Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7fb85653ba00>\n",
      "/home/ayman/anaconda3/envs/ditft/lib/python3.10/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "Patch size = (16, 16)\n",
      "Load ckpt from models/rvlcdip_dit-b.pth\n",
      "Load state_dict by model_key = model\n",
      "Weights of Beit not initialized from pretrained model: ['blocks.0.attn.relative_position_bias_table', 'blocks.1.attn.relative_position_bias_table', 'blocks.2.attn.relative_position_bias_table', 'blocks.3.attn.relative_position_bias_table', 'blocks.4.attn.relative_position_bias_table', 'blocks.5.attn.relative_position_bias_table', 'blocks.6.attn.relative_position_bias_table', 'blocks.7.attn.relative_position_bias_table', 'blocks.8.attn.relative_position_bias_table', 'blocks.9.attn.relative_position_bias_table', 'blocks.10.attn.relative_position_bias_table', 'blocks.11.attn.relative_position_bias_table']\n",
      "Weights from pretrained model not used in Beit: ['pos_embed']\n",
      "Ignored weights of Beit not initialized from pretrained model: ['blocks.0.attn.relative_position_index', 'blocks.1.attn.relative_position_index', 'blocks.2.attn.relative_position_index', 'blocks.3.attn.relative_position_index', 'blocks.4.attn.relative_position_index', 'blocks.5.attn.relative_position_index', 'blocks.6.attn.relative_position_index', 'blocks.7.attn.relative_position_index', 'blocks.8.attn.relative_position_index', 'blocks.9.attn.relative_position_index', 'blocks.10.attn.relative_position_index', 'blocks.11.attn.relative_position_index']\n",
      "Model = Beit(\n",
      "  (patch_embed): PatchEmbed(\n",
      "    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
      "    (norm): Identity()\n",
      "  )\n",
      "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "  (blocks): ModuleList(\n",
      "    (0): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1-11): 11 x Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): DropPath()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): Identity()\n",
      "  (fc_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (head): Linear(in_features=768, out_features=16, bias=True)\n",
      ")\n",
      "number of params: 85774288\n",
      "LR = 0.00050000\n",
      "Batch size = 64\n",
      "Update frequent = 2\n",
      "Number of training examples = 1064\n",
      "Number of training training per epoch = 16\n",
      "Assigned values = [0.023757264018058777, 0.03167635202407837, 0.04223513603210449, 0.056313514709472656, 0.07508468627929688, 0.1001129150390625, 0.13348388671875, 0.177978515625, 0.2373046875, 0.31640625, 0.421875, 0.5625, 0.75, 1.0]\n",
      "Param groups = {\n",
      "  \"layer_0_no_decay\": {\n",
      "    \"weight_decay\": 0.0,\n",
      "    \"params\": [\n",
      "      \"cls_token\",\n",
      "      \"patch_embed.proj.bias\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.023757264018058777\n",
      "  },\n",
      "  \"layer_0_decay\": {\n",
      "    \"weight_decay\": 0.05,\n",
      "    \"params\": [\n",
      "      \"patch_embed.proj.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.023757264018058777\n",
      "  },\n",
      "  \"layer_1_no_decay\": {\n",
      "    \"weight_decay\": 0.0,\n",
      "    \"params\": [\n",
      "      \"blocks.0.gamma_1\",\n",
      "      \"blocks.0.gamma_2\",\n",
      "      \"blocks.0.norm1.weight\",\n",
      "      \"blocks.0.norm1.bias\",\n",
      "      \"blocks.0.attn.q_bias\",\n",
      "      \"blocks.0.attn.v_bias\",\n",
      "      \"blocks.0.attn.proj.bias\",\n",
      "      \"blocks.0.norm2.weight\",\n",
      "      \"blocks.0.norm2.bias\",\n",
      "      \"blocks.0.mlp.fc1.bias\",\n",
      "      \"blocks.0.mlp.fc2.bias\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.03167635202407837\n",
      "  },\n",
      "  \"layer_1_decay\": {\n",
      "    \"weight_decay\": 0.05,\n",
      "    \"params\": [\n",
      "      \"blocks.0.attn.relative_position_bias_table\",\n",
      "      \"blocks.0.attn.qkv.weight\",\n",
      "      \"blocks.0.attn.proj.weight\",\n",
      "      \"blocks.0.mlp.fc1.weight\",\n",
      "      \"blocks.0.mlp.fc2.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.03167635202407837\n",
      "  },\n",
      "  \"layer_2_no_decay\": {\n",
      "    \"weight_decay\": 0.0,\n",
      "    \"params\": [\n",
      "      \"blocks.1.gamma_1\",\n",
      "      \"blocks.1.gamma_2\",\n",
      "      \"blocks.1.norm1.weight\",\n",
      "      \"blocks.1.norm1.bias\",\n",
      "      \"blocks.1.attn.q_bias\",\n",
      "      \"blocks.1.attn.v_bias\",\n",
      "      \"blocks.1.attn.proj.bias\",\n",
      "      \"blocks.1.norm2.weight\",\n",
      "      \"blocks.1.norm2.bias\",\n",
      "      \"blocks.1.mlp.fc1.bias\",\n",
      "      \"blocks.1.mlp.fc2.bias\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.04223513603210449\n",
      "  },\n",
      "  \"layer_2_decay\": {\n",
      "    \"weight_decay\": 0.05,\n",
      "    \"params\": [\n",
      "      \"blocks.1.attn.relative_position_bias_table\",\n",
      "      \"blocks.1.attn.qkv.weight\",\n",
      "      \"blocks.1.attn.proj.weight\",\n",
      "      \"blocks.1.mlp.fc1.weight\",\n",
      "      \"blocks.1.mlp.fc2.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.04223513603210449\n",
      "  },\n",
      "  \"layer_3_no_decay\": {\n",
      "    \"weight_decay\": 0.0,\n",
      "    \"params\": [\n",
      "      \"blocks.2.gamma_1\",\n",
      "      \"blocks.2.gamma_2\",\n",
      "      \"blocks.2.norm1.weight\",\n",
      "      \"blocks.2.norm1.bias\",\n",
      "      \"blocks.2.attn.q_bias\",\n",
      "      \"blocks.2.attn.v_bias\",\n",
      "      \"blocks.2.attn.proj.bias\",\n",
      "      \"blocks.2.norm2.weight\",\n",
      "      \"blocks.2.norm2.bias\",\n",
      "      \"blocks.2.mlp.fc1.bias\",\n",
      "      \"blocks.2.mlp.fc2.bias\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.056313514709472656\n",
      "  },\n",
      "  \"layer_3_decay\": {\n",
      "    \"weight_decay\": 0.05,\n",
      "    \"params\": [\n",
      "      \"blocks.2.attn.relative_position_bias_table\",\n",
      "      \"blocks.2.attn.qkv.weight\",\n",
      "      \"blocks.2.attn.proj.weight\",\n",
      "      \"blocks.2.mlp.fc1.weight\",\n",
      "      \"blocks.2.mlp.fc2.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.056313514709472656\n",
      "  },\n",
      "  \"layer_4_no_decay\": {\n",
      "    \"weight_decay\": 0.0,\n",
      "    \"params\": [\n",
      "      \"blocks.3.gamma_1\",\n",
      "      \"blocks.3.gamma_2\",\n",
      "      \"blocks.3.norm1.weight\",\n",
      "      \"blocks.3.norm1.bias\",\n",
      "      \"blocks.3.attn.q_bias\",\n",
      "      \"blocks.3.attn.v_bias\",\n",
      "      \"blocks.3.attn.proj.bias\",\n",
      "      \"blocks.3.norm2.weight\",\n",
      "      \"blocks.3.norm2.bias\",\n",
      "      \"blocks.3.mlp.fc1.bias\",\n",
      "      \"blocks.3.mlp.fc2.bias\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.07508468627929688\n",
      "  },\n",
      "  \"layer_4_decay\": {\n",
      "    \"weight_decay\": 0.05,\n",
      "    \"params\": [\n",
      "      \"blocks.3.attn.relative_position_bias_table\",\n",
      "      \"blocks.3.attn.qkv.weight\",\n",
      "      \"blocks.3.attn.proj.weight\",\n",
      "      \"blocks.3.mlp.fc1.weight\",\n",
      "      \"blocks.3.mlp.fc2.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.07508468627929688\n",
      "  },\n",
      "  \"layer_5_no_decay\": {\n",
      "    \"weight_decay\": 0.0,\n",
      "    \"params\": [\n",
      "      \"blocks.4.gamma_1\",\n",
      "      \"blocks.4.gamma_2\",\n",
      "      \"blocks.4.norm1.weight\",\n",
      "      \"blocks.4.norm1.bias\",\n",
      "      \"blocks.4.attn.q_bias\",\n",
      "      \"blocks.4.attn.v_bias\",\n",
      "      \"blocks.4.attn.proj.bias\",\n",
      "      \"blocks.4.norm2.weight\",\n",
      "      \"blocks.4.norm2.bias\",\n",
      "      \"blocks.4.mlp.fc1.bias\",\n",
      "      \"blocks.4.mlp.fc2.bias\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.1001129150390625\n",
      "  },\n",
      "  \"layer_5_decay\": {\n",
      "    \"weight_decay\": 0.05,\n",
      "    \"params\": [\n",
      "      \"blocks.4.attn.relative_position_bias_table\",\n",
      "      \"blocks.4.attn.qkv.weight\",\n",
      "      \"blocks.4.attn.proj.weight\",\n",
      "      \"blocks.4.mlp.fc1.weight\",\n",
      "      \"blocks.4.mlp.fc2.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.1001129150390625\n",
      "  },\n",
      "  \"layer_6_no_decay\": {\n",
      "    \"weight_decay\": 0.0,\n",
      "    \"params\": [\n",
      "      \"blocks.5.gamma_1\",\n",
      "      \"blocks.5.gamma_2\",\n",
      "      \"blocks.5.norm1.weight\",\n",
      "      \"blocks.5.norm1.bias\",\n",
      "      \"blocks.5.attn.q_bias\",\n",
      "      \"blocks.5.attn.v_bias\",\n",
      "      \"blocks.5.attn.proj.bias\",\n",
      "      \"blocks.5.norm2.weight\",\n",
      "      \"blocks.5.norm2.bias\",\n",
      "      \"blocks.5.mlp.fc1.bias\",\n",
      "      \"blocks.5.mlp.fc2.bias\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.13348388671875\n",
      "  },\n",
      "  \"layer_6_decay\": {\n",
      "    \"weight_decay\": 0.05,\n",
      "    \"params\": [\n",
      "      \"blocks.5.attn.relative_position_bias_table\",\n",
      "      \"blocks.5.attn.qkv.weight\",\n",
      "      \"blocks.5.attn.proj.weight\",\n",
      "      \"blocks.5.mlp.fc1.weight\",\n",
      "      \"blocks.5.mlp.fc2.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.13348388671875\n",
      "  },\n",
      "  \"layer_7_no_decay\": {\n",
      "    \"weight_decay\": 0.0,\n",
      "    \"params\": [\n",
      "      \"blocks.6.gamma_1\",\n",
      "      \"blocks.6.gamma_2\",\n",
      "      \"blocks.6.norm1.weight\",\n",
      "      \"blocks.6.norm1.bias\",\n",
      "      \"blocks.6.attn.q_bias\",\n",
      "      \"blocks.6.attn.v_bias\",\n",
      "      \"blocks.6.attn.proj.bias\",\n",
      "      \"blocks.6.norm2.weight\",\n",
      "      \"blocks.6.norm2.bias\",\n",
      "      \"blocks.6.mlp.fc1.bias\",\n",
      "      \"blocks.6.mlp.fc2.bias\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.177978515625\n",
      "  },\n",
      "  \"layer_7_decay\": {\n",
      "    \"weight_decay\": 0.05,\n",
      "    \"params\": [\n",
      "      \"blocks.6.attn.relative_position_bias_table\",\n",
      "      \"blocks.6.attn.qkv.weight\",\n",
      "      \"blocks.6.attn.proj.weight\",\n",
      "      \"blocks.6.mlp.fc1.weight\",\n",
      "      \"blocks.6.mlp.fc2.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.177978515625\n",
      "  },\n",
      "  \"layer_8_no_decay\": {\n",
      "    \"weight_decay\": 0.0,\n",
      "    \"params\": [\n",
      "      \"blocks.7.gamma_1\",\n",
      "      \"blocks.7.gamma_2\",\n",
      "      \"blocks.7.norm1.weight\",\n",
      "      \"blocks.7.norm1.bias\",\n",
      "      \"blocks.7.attn.q_bias\",\n",
      "      \"blocks.7.attn.v_bias\",\n",
      "      \"blocks.7.attn.proj.bias\",\n",
      "      \"blocks.7.norm2.weight\",\n",
      "      \"blocks.7.norm2.bias\",\n",
      "      \"blocks.7.mlp.fc1.bias\",\n",
      "      \"blocks.7.mlp.fc2.bias\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.2373046875\n",
      "  },\n",
      "  \"layer_8_decay\": {\n",
      "    \"weight_decay\": 0.05,\n",
      "    \"params\": [\n",
      "      \"blocks.7.attn.relative_position_bias_table\",\n",
      "      \"blocks.7.attn.qkv.weight\",\n",
      "      \"blocks.7.attn.proj.weight\",\n",
      "      \"blocks.7.mlp.fc1.weight\",\n",
      "      \"blocks.7.mlp.fc2.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.2373046875\n",
      "  },\n",
      "  \"layer_9_no_decay\": {\n",
      "    \"weight_decay\": 0.0,\n",
      "    \"params\": [\n",
      "      \"blocks.8.gamma_1\",\n",
      "      \"blocks.8.gamma_2\",\n",
      "      \"blocks.8.norm1.weight\",\n",
      "      \"blocks.8.norm1.bias\",\n",
      "      \"blocks.8.attn.q_bias\",\n",
      "      \"blocks.8.attn.v_bias\",\n",
      "      \"blocks.8.attn.proj.bias\",\n",
      "      \"blocks.8.norm2.weight\",\n",
      "      \"blocks.8.norm2.bias\",\n",
      "      \"blocks.8.mlp.fc1.bias\",\n",
      "      \"blocks.8.mlp.fc2.bias\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.31640625\n",
      "  },\n",
      "  \"layer_9_decay\": {\n",
      "    \"weight_decay\": 0.05,\n",
      "    \"params\": [\n",
      "      \"blocks.8.attn.relative_position_bias_table\",\n",
      "      \"blocks.8.attn.qkv.weight\",\n",
      "      \"blocks.8.attn.proj.weight\",\n",
      "      \"blocks.8.mlp.fc1.weight\",\n",
      "      \"blocks.8.mlp.fc2.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.31640625\n",
      "  },\n",
      "  \"layer_10_no_decay\": {\n",
      "    \"weight_decay\": 0.0,\n",
      "    \"params\": [\n",
      "      \"blocks.9.gamma_1\",\n",
      "      \"blocks.9.gamma_2\",\n",
      "      \"blocks.9.norm1.weight\",\n",
      "      \"blocks.9.norm1.bias\",\n",
      "      \"blocks.9.attn.q_bias\",\n",
      "      \"blocks.9.attn.v_bias\",\n",
      "      \"blocks.9.attn.proj.bias\",\n",
      "      \"blocks.9.norm2.weight\",\n",
      "      \"blocks.9.norm2.bias\",\n",
      "      \"blocks.9.mlp.fc1.bias\",\n",
      "      \"blocks.9.mlp.fc2.bias\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.421875\n",
      "  },\n",
      "  \"layer_10_decay\": {\n",
      "    \"weight_decay\": 0.05,\n",
      "    \"params\": [\n",
      "      \"blocks.9.attn.relative_position_bias_table\",\n",
      "      \"blocks.9.attn.qkv.weight\",\n",
      "      \"blocks.9.attn.proj.weight\",\n",
      "      \"blocks.9.mlp.fc1.weight\",\n",
      "      \"blocks.9.mlp.fc2.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.421875\n",
      "  },\n",
      "  \"layer_11_no_decay\": {\n",
      "    \"weight_decay\": 0.0,\n",
      "    \"params\": [\n",
      "      \"blocks.10.gamma_1\",\n",
      "      \"blocks.10.gamma_2\",\n",
      "      \"blocks.10.norm1.weight\",\n",
      "      \"blocks.10.norm1.bias\",\n",
      "      \"blocks.10.attn.q_bias\",\n",
      "      \"blocks.10.attn.v_bias\",\n",
      "      \"blocks.10.attn.proj.bias\",\n",
      "      \"blocks.10.norm2.weight\",\n",
      "      \"blocks.10.norm2.bias\",\n",
      "      \"blocks.10.mlp.fc1.bias\",\n",
      "      \"blocks.10.mlp.fc2.bias\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.5625\n",
      "  },\n",
      "  \"layer_11_decay\": {\n",
      "    \"weight_decay\": 0.05,\n",
      "    \"params\": [\n",
      "      \"blocks.10.attn.relative_position_bias_table\",\n",
      "      \"blocks.10.attn.qkv.weight\",\n",
      "      \"blocks.10.attn.proj.weight\",\n",
      "      \"blocks.10.mlp.fc1.weight\",\n",
      "      \"blocks.10.mlp.fc2.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.5625\n",
      "  },\n",
      "  \"layer_12_no_decay\": {\n",
      "    \"weight_decay\": 0.0,\n",
      "    \"params\": [\n",
      "      \"blocks.11.gamma_1\",\n",
      "      \"blocks.11.gamma_2\",\n",
      "      \"blocks.11.norm1.weight\",\n",
      "      \"blocks.11.norm1.bias\",\n",
      "      \"blocks.11.attn.q_bias\",\n",
      "      \"blocks.11.attn.v_bias\",\n",
      "      \"blocks.11.attn.proj.bias\",\n",
      "      \"blocks.11.norm2.weight\",\n",
      "      \"blocks.11.norm2.bias\",\n",
      "      \"blocks.11.mlp.fc1.bias\",\n",
      "      \"blocks.11.mlp.fc2.bias\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.75\n",
      "  },\n",
      "  \"layer_12_decay\": {\n",
      "    \"weight_decay\": 0.05,\n",
      "    \"params\": [\n",
      "      \"blocks.11.attn.relative_position_bias_table\",\n",
      "      \"blocks.11.attn.qkv.weight\",\n",
      "      \"blocks.11.attn.proj.weight\",\n",
      "      \"blocks.11.mlp.fc1.weight\",\n",
      "      \"blocks.11.mlp.fc2.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.75\n",
      "  },\n",
      "  \"layer_13_no_decay\": {\n",
      "    \"weight_decay\": 0.0,\n",
      "    \"params\": [\n",
      "      \"fc_norm.weight\",\n",
      "      \"fc_norm.bias\",\n",
      "      \"head.bias\"\n",
      "    ],\n",
      "    \"lr_scale\": 1.0\n",
      "  },\n",
      "  \"layer_13_decay\": {\n",
      "    \"weight_decay\": 0.05,\n",
      "    \"params\": [\n",
      "      \"head.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 1.0\n",
      "  }\n",
      "}\n",
      "Use step level LR scheduler!\n",
      "Set warmup steps = 320\n",
      "Set warmup steps = 0\n",
      "Max WD = 0.0500000, Min WD = 0.0500000\n",
      "criterion = LabelSmoothingCrossEntropy()\n",
      "Auto resume checkpoint: output/dit-base-rvlcdip-test/checkpoint-49.pth\n",
      "Resume checkpoint output/dit-base-rvlcdip-test/checkpoint-49.pth\n",
      "With optim & sched!\n",
      "Start training for 50 epochs\n",
      "Training time 0:00:00\n"
     ]
    }
   ],
   "source": [
    "# Must be in the following path to do DIT inferences\n",
    "## EVALUATION\n",
    "# !c4c --master_port=47770  run_class_finetuning.py --model beit_base_patch16_224 --data_path \"/content/rvlcdip_test/\" --eval_data_path \"/content/rvlcdip_test/\" --enable_deepspeed --nb_classes 16 --eval --data_set rvlcdip --finetune /content/dit-base-224-p16-500k-62d53a.zip --output_dir /content/output_dir --log_dir /content/output_dir/tf --batch_size 256 --abs_pos_emb --disable_rel_pos_bias\n",
    "%cd /home/ayman/Downloads/projects/di_ft/unilm/dit/classification/\n",
    "## FT CLASSIFIER\n",
    "!mkdir -p output/dit-base-rvlcdip-test\n",
    "c4c --master_port=47770  run_class_finetuning.py# --data_set rvlcdip: solo esta implementado este, adaptar dataset de entrada a este formato\n",
    "\n",
    "!python run_class_finetuning.py \\\n",
    "    --num_workers 0 \\\n",
    "    --model beit_base_patch16_224 \\\n",
    "    --data_path \"rvlcdip_test_small/\" \\\n",
    "    --eval_data_path \"rvlcdip_test_small/\" \\\n",
    "    --nb_classes 16 \\\n",
    "    --data_set rvlcdip \\\n",
    "    --finetune models/rvlcdip_dit-b.pth \\\n",
    "    --output_dir output/dit-base-rvlcdip-test/  \\\n",
    "    --log_dir output/dit-base-rvlcdip-test/tf \\\n",
    "    --batch_size 32 \\\n",
    "    --lr 5e-4 \\\n",
    "    --update_freq 2 \\\n",
    "    --eval_freq 10 \\\n",
    "    --save_ckpt_freq 10 \\\n",
    "    --warmup_epochs 20 \\\n",
    "    --epochs 50 \\\n",
    "    --layer_scale_init_value 1e-5 \\\n",
    "    --layer_decay 0.75 \\\n",
    "    --drop_path 0.2  \\\n",
    "    --weight_decay 0.05 \\\n",
    "    --clip_grad 1.0 \\\n",
    "    --abs_pos_emb \\\n",
    "    --disable_rel_pos_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "JcsY2-wsxbDG",
    "outputId": "2ed7be40-2ee5-4c2c-c953-6bf901185530"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_a8d8b090-9ca7-4aad-bf97-6079528e845d\", \"dit-base-rvlcdip-test.zip\", 22)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%cd /content\n",
    "from google.colab import files\n",
    "import shutil\n",
    "\n",
    "output_path = \"/content/output/\" + exp_name\n",
    "zip_path = exp_name + \".zip\"\n",
    "\n",
    "shutil.make_archive(exp_name, 'zip', output_path)\n",
    "files.download(zip_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1DhejbusPxIF"
   },
   "source": [
    "##2.2. Source code execution (TODO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qrPFuDtOUaqz"
   },
   "source": [
    "# 3. Inference and model to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ayman/Downloads/projects/di_ft/unilm/dit/classification\n"
     ]
    }
   ],
   "source": [
    "%cd unilm/dit/classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "CV7FTX02pmBw"
   },
   "outputs": [],
   "source": [
    "model_path = \"models/rvlcdip_dit-l.pth\" # Original model\n",
    "#model_path = \"output/dit-base-rvlcdip-test/checkpoint-best.pth\" # FT model\n",
    "\n",
    "# Check FT variables\n",
    "model_ini = \"beit_base_patch16_224\"\n",
    "nb_classes = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VmMaeSHs9vFP"
   },
   "source": [
    "## 3.1. Torch Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ednu7q9jMYuL",
    "outputId": "6278fead-c11a-4097-d4d0-2641d6e610ed"
   },
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 12.00 MiB (GPU 0; 5.81 GiB total capacity; 4.66 GiB already allocated; 9.44 MiB free; 5.29 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 15\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtimm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_model\n\u001b[1;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39m create_model(\n\u001b[1;32m      7\u001b[0m     model_ini,\n\u001b[1;32m      8\u001b[0m     pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m     attn_drop_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m,\n\u001b[1;32m     13\u001b[0m     drop_block_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,)\n\u001b[0;32m---> 15\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint, strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# model.eval()\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ditft/lib/python3.10/site-packages/torch/serialization.py:809\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    808\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 809\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    810\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights_only:\n\u001b[1;32m    811\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/ditft/lib/python3.10/site-packages/torch/serialization.py:1172\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1170\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1171\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[0;32m-> 1172\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1174\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/envs/ditft/lib/python3.10/site-packages/torch/serialization.py:1142\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1141\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1142\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[0;32m~/anaconda3/envs/ditft/lib/python3.10/site-packages/torch/serialization.py:1116\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1112\u001b[0m storage \u001b[38;5;241m=\u001b[39m zip_file\u001b[38;5;241m.\u001b[39mget_storage_from_record(name, numel, torch\u001b[38;5;241m.\u001b[39mUntypedStorage)\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_untyped_storage\n\u001b[1;32m   1113\u001b[0m \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m   1114\u001b[0m \u001b[38;5;66;03m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[1;32m   1115\u001b[0m typed_storage \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstorage\u001b[38;5;241m.\u001b[39mTypedStorage(\n\u001b[0;32m-> 1116\u001b[0m     wrap_storage\u001b[38;5;241m=\u001b[39m\u001b[43mrestore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1117\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m   1118\u001b[0m     _internal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typed_storage\u001b[38;5;241m.\u001b[39m_data_ptr() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1121\u001b[0m     loaded_storages[key] \u001b[38;5;241m=\u001b[39m typed_storage\n",
      "File \u001b[0;32m~/anaconda3/envs/ditft/lib/python3.10/site-packages/torch/serialization.py:217\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_restore_location\u001b[39m(storage, location):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, _, fn \u001b[38;5;129;01min\u001b[39;00m _package_registry:\n\u001b[0;32m--> 217\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    219\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/envs/ditft/lib/python3.10/site-packages/torch/serialization.py:187\u001b[0m, in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mUntypedStorage(obj\u001b[38;5;241m.\u001b[39mnbytes(), device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(location))\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 187\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ditft/lib/python3.10/site-packages/torch/_utils.py:81\u001b[0m, in \u001b[0;36m_cuda\u001b[0;34m(self, device, non_blocking, **kwargs)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m new_type(indices, values, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m     untyped_storage \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUntypedStorage\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m     untyped_storage\u001b[38;5;241m.\u001b[39mcopy_(\u001b[38;5;28mself\u001b[39m, non_blocking)\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m untyped_storage\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 0; 5.81 GiB total capacity; 4.66 GiB already allocated; 9.44 MiB free; 5.29 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import cv2\n",
    "from timm.models import create_model\n",
    "\n",
    "model = create_model(\n",
    "    model_ini,\n",
    "    pretrained=False,\n",
    "    num_classes=nb_classes,\n",
    "    drop_rate=0.0,\n",
    "    drop_path_rate=0.1,\n",
    "    attn_drop_rate=0.0,\n",
    "    drop_block_rate=None,)\n",
    "\n",
    "checkpoint = torch.load(model_path)\n",
    "model.load_state_dict(checkpoint, strict=False)\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o4qis21ZUXdL"
   },
   "outputs": [],
   "source": [
    "def dit_classifier_torch_inference(img_path, model):\n",
    "  input = cv2.imread(img_path)\n",
    "  input = cv2.resize(input, (224, 224))\n",
    "  input_tensor = torch.from_numpy(input.transpose((2, 0, 1))).unsqueeze(0).float()\n",
    "  # input_tensor = torch.randn(1, 3, 224, 224) # dummy_input\n",
    "\n",
    "  with torch.no_grad():\n",
    "      outputs = model(input_tensor)\n",
    "\n",
    "  print(outputs)\n",
    "  output_tensor = outputs[0]\n",
    "  score, predicted_class = torch.max(output_tensor, dim=0)\n",
    "  predicted_class_name = list(class_dict.keys())[list(class_dict.values()).index(predicted_class.item())]\n",
    "    \n",
    "\n",
    "  return predicted_class_name, score.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DcqjxJ-FOHJ6",
    "outputId": "a56be486-1917-4f26-b6c1-b8c64565a1a8"
   },
   "outputs": [],
   "source": [
    "  img_path = \"rvlcdip_test_small/images/advertisement/0000001863.tif\"\n",
    "\n",
    "  pred_class, score = dit_classifier_torch_inference(img_path, model)\n",
    "\n",
    "  print(\"Predicted Class:\", pred_class)\n",
    "  print(\"Score:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "WpT9MG1jNsxr",
    "outputId": "52b81a8e-8a68-40f6-d808-2946b32b1a6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing:  rvlcdip_test_small/images/memo/0000137752.tif\n",
      "tensor([[ 7.8088e-04, -3.7980e-04, -1.1745e-03, -1.2777e-08, -7.5884e-05,\n",
      "          9.4014e-04, -1.2564e-04, -1.1089e-03,  5.6615e-04, -1.0927e-03,\n",
      "         -6.3653e-04, -1.7826e-04,  6.5872e-04, -2.2236e-04,  1.1446e-03,\n",
      "          3.8610e-04]])\n",
      "   Class: resume\n",
      "   Score: 0.0011445711134001613\n",
      "\n",
      "Processing:  rvlcdip_test_small/images/memo/0000128853.tif\n",
      "tensor([[ 7.8023e-04, -3.8789e-04, -1.1596e-03,  6.6246e-06, -7.3709e-05,\n",
      "          9.2846e-04, -1.2773e-04, -1.1024e-03,  5.6532e-04, -1.0821e-03,\n",
      "         -6.2381e-04, -1.7169e-04,  6.5692e-04, -2.2439e-04,  1.1428e-03,\n",
      "          3.8395e-04]])\n",
      "   Class: resume\n",
      "   Score: 0.0011427641147747636\n",
      "\n",
      "Processing:  rvlcdip_test_small/images/memo/0000047727.tif\n",
      "tensor([[ 7.5751e-04, -3.8416e-04, -1.1538e-03,  9.8887e-06, -6.7158e-05,\n",
      "          8.9883e-04, -1.1628e-04, -1.1219e-03,  5.6307e-04, -1.0426e-03,\n",
      "         -6.0846e-04, -1.6525e-04,  6.5640e-04, -2.1334e-04,  1.1208e-03,\n",
      "          3.9173e-04]])\n",
      "   Class: resume\n",
      "   Score: 0.0011208249488845468\n",
      "\n",
      "Processing:  rvlcdip_test_small/images/memo/0000084686.tif\n",
      "tensor([[ 7.7329e-04, -3.7024e-04, -1.1412e-03, -3.8537e-05, -7.8095e-05,\n",
      "          9.4815e-04, -1.0981e-04, -1.1204e-03,  5.5501e-04, -1.0592e-03,\n",
      "         -6.0597e-04, -1.6541e-04,  6.4104e-04, -2.2315e-04,  1.1342e-03,\n",
      "          3.8348e-04]])\n",
      "   Class: resume\n",
      "   Score: 0.001134220976382494\n",
      "\n",
      "Processing:  rvlcdip_test_small/images/memo/0000168686.tif\n",
      "tensor([[ 7.7198e-04, -4.0365e-04, -1.1747e-03, -2.8421e-06, -7.5314e-05,\n",
      "          9.3142e-04, -1.3763e-04, -1.1076e-03,  5.7266e-04, -1.0798e-03,\n",
      "         -6.2509e-04, -1.6915e-04,  6.5917e-04, -2.2066e-04,  1.1393e-03,\n",
      "          3.8483e-04]])\n",
      "   Class: resume\n",
      "   Score: 0.0011393418535590172\n",
      "\n",
      "Processing:  rvlcdip_test_small/images/memo/0000140395.tif\n",
      "tensor([[ 7.8864e-04, -3.8591e-04, -1.1674e-03,  3.8643e-06, -8.3584e-05,\n",
      "          9.1966e-04, -1.3310e-04, -1.1109e-03,  5.7509e-04, -1.0801e-03,\n",
      "         -6.2676e-04, -1.6150e-04,  6.6882e-04, -2.2590e-04,  1.1418e-03,\n",
      "          3.7390e-04]])\n",
      "   Class: resume\n",
      "   Score: 0.0011418232461437583\n",
      "\n",
      "Processing:  rvlcdip_test_small/images/memo/0000085691.tif\n",
      "tensor([[ 7.8724e-04, -3.7545e-04, -1.1740e-03, -1.2308e-06, -7.3351e-05,\n",
      "          9.2739e-04, -1.2748e-04, -1.1072e-03,  5.7150e-04, -1.0758e-03,\n",
      "         -6.2379e-04, -1.7368e-04,  6.6203e-04, -2.2379e-04,  1.1529e-03,\n",
      "          3.8923e-04]])\n",
      "   Class: resume\n",
      "   Score: 0.0011529147159308195\n",
      "\n",
      "Processing:  rvlcdip_test_small/images/memo/0000177797.tif\n",
      "tensor([[ 7.7660e-04, -3.7198e-04, -1.1694e-03, -1.4519e-05, -6.8361e-05,\n",
      "          9.4498e-04, -1.1299e-04, -1.1121e-03,  5.7488e-04, -1.0459e-03,\n",
      "         -6.2375e-04, -1.6245e-04,  6.5900e-04, -2.0514e-04,  1.1484e-03,\n",
      "          3.8606e-04]])\n",
      "   Class: resume\n",
      "   Score: 0.001148395356722176\n",
      "\n",
      "Processing:  rvlcdip_test_small/images/memo/0000115583.tif\n",
      "tensor([[ 7.6078e-04, -3.8482e-04, -1.1641e-03, -1.0025e-05, -8.6532e-05,\n",
      "          9.3153e-04, -1.1776e-04, -1.1151e-03,  5.7277e-04, -1.0658e-03,\n",
      "         -6.0688e-04, -1.7129e-04,  6.5096e-04, -2.2003e-04,  1.1506e-03,\n",
      "          3.8390e-04]])\n",
      "   Class: resume\n",
      "   Score: 0.0011505760485306382\n",
      "\n",
      "Processing:  rvlcdip_test_small/images/memo/0000163813.tif\n",
      "tensor([[ 7.8460e-04, -3.7818e-04, -1.1425e-03,  2.4784e-05, -5.3395e-05,\n",
      "          9.1846e-04, -1.0302e-04, -1.1469e-03,  5.4746e-04, -1.0605e-03,\n",
      "         -6.2300e-04, -1.0844e-04,  6.3927e-04, -2.1933e-04,  1.1605e-03,\n",
      "          3.8773e-04]])\n",
      "   Class: resume\n",
      "   Score: 0.0011605208273977041\n",
      "\n",
      "Processing:  rvlcdip_test_small/images/memo/0000170520.tif\n",
      "tensor([[ 7.7070e-04, -3.7464e-04, -1.1624e-03, -1.9367e-06, -9.4571e-05,\n",
      "          9.3351e-04, -1.1449e-04, -1.1236e-03,  5.7445e-04, -1.0609e-03,\n",
      "         -6.0309e-04, -1.5824e-04,  6.4982e-04, -2.2941e-04,  1.1451e-03,\n",
      "          3.7586e-04]])\n",
      "   Class: resume\n",
      "   Score: 0.0011450678575783968\n",
      "\n",
      "Processing:  rvlcdip_test_small/images/memo/0000052023.tif\n",
      "tensor([[ 7.7756e-04, -3.7617e-04, -1.1682e-03,  7.0248e-06, -8.6366e-05,\n",
      "          9.2941e-04, -1.2448e-04, -1.1072e-03,  5.7621e-04, -1.0819e-03,\n",
      "         -6.1626e-04, -1.9053e-04,  6.4985e-04, -2.2487e-04,  1.1434e-03,\n",
      "          3.9007e-04]])\n",
      "   Class: resume\n",
      "   Score: 0.0011434446787461638\n",
      "\n",
      "Processing:  rvlcdip_test_small/images/memo/0000005751.tif\n",
      "tensor([[ 7.8229e-04, -3.6675e-04, -1.1603e-03,  3.2013e-05, -6.0650e-05,\n",
      "          9.0718e-04, -1.3153e-04, -1.1267e-03,  5.4772e-04, -1.0524e-03,\n",
      "         -6.2131e-04, -1.1280e-04,  6.4808e-04, -2.2889e-04,  1.1531e-03,\n",
      "          3.8616e-04]])\n",
      "   Class: resume\n",
      "   Score: 0.001153094694018364\n",
      "\n",
      "Processing:  rvlcdip_test_small/images/memo/0000211867.tif\n",
      "tensor([[ 7.8020e-04, -3.9038e-04, -1.1652e-03,  2.2902e-05, -5.9929e-05,\n",
      "          9.1601e-04, -1.1159e-04, -1.1117e-03,  5.7675e-04, -1.0332e-03,\n",
      "         -5.9033e-04, -1.4946e-04,  6.4049e-04, -2.1611e-04,  1.1606e-03,\n",
      "          3.8663e-04]])\n",
      "   Class: resume\n",
      "   Score: 0.0011606287444010377\n",
      "\n",
      "Processing:  rvlcdip_test_small/images/memo/0000146569.tif\n",
      "tensor([[ 7.8145e-04, -3.8451e-04, -1.1563e-03,  5.5372e-06, -7.3851e-05,\n",
      "          9.5174e-04, -1.1898e-04, -1.1174e-03,  5.5930e-04, -1.0730e-03,\n",
      "         -6.1209e-04, -1.7070e-04,  6.5254e-04, -2.2650e-04,  1.1570e-03,\n",
      "          3.8626e-04]])\n",
      "   Class: resume\n",
      "   Score: 0.0011570397764444351\n",
      "\n",
      "Processing:  rvlcdip_test_small/images/memo/0000054909.tif\n",
      "tensor([[ 7.8901e-04, -3.6912e-04, -1.1587e-03,  1.3448e-06, -7.6811e-05,\n",
      "          9.4162e-04, -1.1871e-04, -1.1146e-03,  5.6222e-04, -1.1084e-03,\n",
      "         -6.3156e-04, -1.8725e-04,  6.5918e-04, -2.4294e-04,  1.1434e-03,\n",
      "          3.7442e-04]])\n",
      "   Class: resume\n",
      "   Score: 0.0011433518957346678\n",
      "\n",
      "Processing:  rvlcdip_test_small/images/memo/0000147146.tif\n",
      "tensor([[ 7.8527e-04, -3.6524e-04, -1.1682e-03, -1.8250e-06, -9.5268e-05,\n",
      "          9.3337e-04, -1.1064e-04, -1.1079e-03,  5.7890e-04, -1.0560e-03,\n",
      "         -5.9357e-04, -1.6903e-04,  6.4951e-04, -2.2684e-04,  1.1480e-03,\n",
      "          3.7241e-04]])\n",
      "   Class: resume\n",
      "   Score: 0.0011479879030957818\n",
      "\n",
      "Processing:  rvlcdip_test_small/images/memo/0000072858.tif\n",
      "tensor([[ 7.8528e-04, -3.8226e-04, -1.1691e-03,  2.1872e-05, -4.4239e-05,\n",
      "          9.3554e-04, -1.1361e-04, -1.1370e-03,  5.4569e-04, -1.0577e-03,\n",
      "         -6.1602e-04, -1.0288e-04,  6.4057e-04, -2.1649e-04,  1.1605e-03,\n",
      "          3.9121e-04]])\n",
      "   Class: resume\n",
      "   Score: 0.0011605211766436696\n",
      "\n",
      "Processing:  rvlcdip_test_small/images/memo/0000036806.tif\n",
      "tensor([[ 7.7667e-04, -4.0104e-04, -1.1570e-03,  2.6359e-05, -7.4064e-05,\n",
      "          9.2818e-04, -1.2072e-04, -1.1031e-03,  5.6486e-04, -1.0767e-03,\n",
      "         -6.2529e-04, -1.5200e-04,  6.7652e-04, -1.9727e-04,  1.1542e-03,\n",
      "          3.9919e-04]])\n",
      "   Class: resume\n",
      "   Score: 0.0011541942367330194\n",
      "\n",
      "Processing:  rvlcdip_test_small/images/memo/0000049423.tif\n",
      "tensor([[ 7.6392e-04, -3.8631e-04, -1.1165e-03,  2.9102e-05, -6.6090e-05,\n",
      "          9.0857e-04, -1.0990e-04, -1.1091e-03,  5.5652e-04, -1.0837e-03,\n",
      "         -6.3981e-04, -1.2662e-04,  6.7623e-04, -2.0453e-04,  1.1099e-03,\n",
      "          3.9526e-04]])\n",
      "   Class: resume\n",
      "   Score: 0.0011098930845037103\n",
      "\n",
      "Processing:  rvlcdip_test_small/images/memo/0000076712.tif\n",
      "tensor([[ 7.7640e-04, -3.8492e-04, -1.1584e-03, -8.7331e-06, -8.5025e-05,\n",
      "          9.2731e-04, -1.2953e-04, -1.1110e-03,  5.7140e-04, -1.0906e-03,\n",
      "         -6.1579e-04, -1.8278e-04,  6.4942e-04, -2.3000e-04,  1.1285e-03,\n",
      "          3.6969e-04]])\n",
      "   Class: resume\n",
      "   Score: 0.0011285326909273863\n",
      "\n",
      "Processing:  rvlcdip_test_small/images/memo/0000109708.tif\n",
      "tensor([[ 7.8727e-04, -3.9228e-04, -1.1542e-03,  3.4091e-05, -6.9975e-05,\n",
      "          9.0294e-04, -1.1923e-04, -1.0921e-03,  5.4983e-04, -1.0485e-03,\n",
      "         -6.1172e-04, -1.5311e-04,  6.4471e-04, -2.2079e-04,  1.1159e-03,\n",
      "          3.7353e-04]])\n",
      "   Class: resume\n",
      "   Score: 0.0011158722918480635\n",
      "\n",
      "Processing:  rvlcdip_test_small/images/memo/0000083868.tif\n",
      "tensor([[ 7.9915e-04, -3.5964e-04, -1.1179e-03,  2.4866e-05, -7.7308e-05,\n",
      "          9.4464e-04, -8.9624e-05, -1.1015e-03,  5.5295e-04, -1.0971e-03,\n",
      "         -6.0919e-04, -1.6053e-04,  6.5289e-04, -2.3064e-04,  1.1372e-03,\n",
      "          3.9831e-04]])\n",
      "   Class: resume\n",
      "   Score: 0.0011372229782864451\n",
      "\n",
      "Processing:  rvlcdip_test_small/images/memo/0000072432.tif\n",
      "tensor([[ 8.1433e-04, -3.4931e-04, -1.1432e-03, -3.9014e-05, -7.9895e-05,\n",
      "          9.2103e-04, -9.0869e-05, -1.1549e-03,  5.4028e-04, -1.0174e-03,\n",
      "         -5.7501e-04, -1.2769e-04,  6.6133e-04, -2.4555e-04,  1.1810e-03,\n",
      "          3.6041e-04]])\n",
      "   Class: resume\n",
      "   Score: 0.0011809696443378925\n",
      "\n",
      "Processing:  rvlcdip_test_small/images/memo/0000066230.tif\n",
      "tensor([[ 7.7966e-04, -3.9280e-04, -1.1500e-03,  1.9871e-05, -6.8470e-05,\n",
      "          9.1978e-04, -1.1982e-04, -1.1142e-03,  5.5672e-04, -1.0702e-03,\n",
      "         -6.2560e-04, -1.2933e-04,  6.4447e-04, -2.2118e-04,  1.1432e-03,\n",
      "          3.8077e-04]])\n",
      "   Class: resume\n",
      "   Score: 0.001143167377449572\n",
      "\n",
      "Processing:  rvlcdip_test_small/images/memo/0000104981.tif\n",
      "tensor([[ 7.6650e-04, -3.8653e-04, -1.1716e-03,  2.7786e-06, -8.1823e-05,\n",
      "          9.2922e-04, -1.2196e-04, -1.1090e-03,  5.7843e-04, -1.0675e-03,\n",
      "         -6.1303e-04, -1.6333e-04,  6.6379e-04, -2.3206e-04,  1.1507e-03,\n",
      "          3.7611e-04]])\n",
      "   Class: resume\n",
      "   Score: 0.0011507085291668773\n",
      "\n",
      "Processing:  rvlcdip_test_small/images/memo/0000211934.tif\n",
      "tensor([[ 7.9426e-04, -4.0070e-04, -1.1608e-03,  5.5549e-06, -4.6033e-05,\n",
      "          8.9790e-04, -1.1591e-04, -1.1239e-03,  5.7238e-04, -1.0795e-03,\n",
      "         -6.1602e-04, -1.4692e-04,  6.4273e-04, -2.2282e-04,  1.1308e-03,\n",
      "          3.8383e-04]])\n",
      "   Class: resume\n",
      "   Score: 0.0011307761305943131\n",
      "\n",
      "Processing:  rvlcdip_test_small/images/memo/0000039302.tif\n",
      "tensor([[ 7.7306e-04, -3.9826e-04, -1.1519e-03, -2.9722e-05, -7.4992e-05,\n",
      "          9.3509e-04, -1.0758e-04, -1.1115e-03,  5.6924e-04, -1.0532e-03,\n",
      "         -6.1131e-04, -1.6205e-04,  6.4769e-04, -2.0508e-04,  1.1241e-03,\n",
      "          3.9275e-04]])\n",
      "   Class: resume\n",
      "   Score: 0.0011241091415286064\n",
      "\n",
      "Processing:  rvlcdip_test_small/images/memo/0000061681.tif\n",
      "tensor([[ 7.9351e-04, -3.8214e-04, -1.1659e-03,  2.4772e-05, -5.1617e-05,\n",
      "          9.1742e-04, -1.2295e-04, -1.1395e-03,  5.5585e-04, -1.0599e-03,\n",
      "         -6.3261e-04, -1.0079e-04,  6.4814e-04, -2.2495e-04,  1.1636e-03,\n",
      "          3.8960e-04]])\n",
      "   Class: resume\n",
      "   Score: 0.00116362189874053\n",
      "\n",
      "Processing:  rvlcdip_test_small/images/memo/0000053543.tif\n",
      "tensor([[ 7.8804e-04, -3.6929e-04, -1.1594e-03,  4.5520e-06, -7.1463e-05,\n",
      "          8.8989e-04, -1.1998e-04, -1.1212e-03,  5.5868e-04, -1.0455e-03,\n",
      "         -5.9585e-04, -1.5422e-04,  6.5942e-04, -2.4706e-04,  1.1221e-03,\n",
      "          3.7700e-04]])\n",
      "   Class: resume\n",
      "   Score: 0.0011220761807635427\n",
      "\n",
      "Processing:  rvlcdip_test_small/images/memo/0000137069.tif\n",
      "tensor([[ 7.9931e-04, -3.8893e-04, -1.1551e-03,  2.1474e-05, -1.0638e-04,\n",
      "          9.4883e-04, -1.2262e-04, -1.1050e-03,  5.5148e-04, -1.1020e-03,\n",
      "         -6.4384e-04, -1.7812e-04,  6.5386e-04, -2.2547e-04,  1.1445e-03,\n",
      "          3.7775e-04]])\n",
      "   Class: resume\n",
      "   Score: 0.0011444841511547565\n",
      "\n",
      "Processing:  rvlcdip_test_small/images/memo/0000177528.tif\n",
      "tensor([[ 8.1341e-04, -3.7974e-04, -1.1578e-03, -2.3094e-05, -1.0927e-04,\n",
      "          9.4696e-04, -1.2060e-04, -1.1192e-03,  5.6141e-04, -1.0988e-03,\n",
      "         -6.2909e-04, -1.8337e-04,  6.8583e-04, -2.3781e-04,  1.1412e-03,\n",
      "          3.6366e-04]])\n",
      "   Class: resume\n",
      "   Score: 0.0011412347666919231\n",
      "\n",
      "Processing:  rvlcdip_test_small/images/memo/0000131294.tif\n",
      "tensor([[ 7.7300e-04, -3.9088e-04, -1.1712e-03,  1.1567e-06, -7.5557e-05,\n",
      "          9.3060e-04, -1.2317e-04, -1.1080e-03,  5.7932e-04, -1.0757e-03,\n",
      "         -6.2573e-04, -1.6156e-04,  6.5141e-04, -2.2196e-04,  1.1443e-03,\n",
      "          3.9217e-04]])\n",
      "   Class: resume\n",
      "   Score: 0.0011442677350714803\n",
      "\n",
      "Processing:  rvlcdip_test_small/images/memo/0000018907.tif\n",
      "tensor([[ 7.7110e-04, -3.7889e-04, -1.1615e-03,  1.3435e-06, -8.4715e-05,\n",
      "          9.2610e-04, -1.2180e-04, -1.1100e-03,  5.6341e-04, -1.0726e-03,\n",
      "         -6.0836e-04, -1.7214e-04,  6.4421e-04, -2.2891e-04,  1.1289e-03,\n",
      "          3.8523e-04]])\n",
      "   Class: resume\n",
      "   Score: 0.0011288933455944061\n",
      "\n",
      "Processing:  rvlcdip_test_small/images/memo/0000066578.tif\n",
      "tensor([[ 7.8537e-04, -3.8870e-04, -1.1646e-03,  2.1618e-05, -6.3978e-05,\n",
      "          9.0071e-04, -1.3270e-04, -1.1162e-03,  5.6164e-04, -1.0719e-03,\n",
      "         -6.2490e-04, -1.1567e-04,  6.4944e-04, -2.2387e-04,  1.1432e-03,\n",
      "          3.8386e-04]])\n",
      "   Class: resume\n",
      "   Score: 0.0011432187166064978\n",
      "\n",
      "Processing:  rvlcdip_test_small/images/memo/00001825.tif\n",
      "tensor([[ 7.9534e-04, -3.7111e-04, -1.1540e-03, -5.2811e-05, -9.2508e-05,\n",
      "          9.6832e-04, -9.8746e-05, -1.1316e-03,  5.6353e-04, -1.0403e-03,\n",
      "         -6.0681e-04, -1.7516e-04,  6.5102e-04, -2.1319e-04,  1.1588e-03,\n",
      "          3.6605e-04]])\n",
      "   Class: resume\n",
      "   Score: 0.0011587721528485417\n",
      "\n",
      "Processing:  rvlcdip_test_small/images/memo/0000044406.tif\n",
      "tensor([[ 7.7683e-04, -3.7120e-04, -1.1657e-03, -3.3370e-06, -8.7126e-05,\n",
      "          9.3197e-04, -1.2075e-04, -1.1189e-03,  5.7673e-04, -1.0666e-03,\n",
      "         -6.1951e-04, -1.6652e-04,  6.5666e-04, -2.2891e-04,  1.1472e-03,\n",
      "          3.7275e-04]])\n",
      "   Class: resume\n",
      "   Score: 0.001147191971540451\n",
      "\n",
      "Processing:  rvlcdip_test_small/images/memo/0000211046.tif\n",
      "tensor([[ 7.7633e-04, -3.6508e-04, -1.1725e-03, -3.8966e-06, -7.6518e-05,\n",
      "          9.3796e-04, -1.1178e-04, -1.1265e-03,  5.7032e-04, -1.0359e-03,\n",
      "         -5.9876e-04, -1.6402e-04,  6.4985e-04, -2.1535e-04,  1.1727e-03,\n",
      "          3.8158e-04]])\n",
      "   Class: resume\n",
      "   Score: 0.0011727125383913517\n",
      "\n",
      "Processing:  rvlcdip_test_small/images/memo/0000075801.tif\n",
      "tensor([[ 7.9309e-04, -3.8439e-04, -1.1432e-03, -7.3994e-07, -4.5656e-05,\n",
      "          9.2064e-04, -1.0808e-04, -1.1298e-03,  5.4368e-04, -1.0536e-03,\n",
      "         -6.4141e-04, -9.1699e-05,  6.5652e-04, -2.0586e-04,  1.1413e-03,\n",
      "          3.8932e-04]])\n",
      "   Class: resume\n",
      "   Score: 0.0011413466418161988\n",
      "\n",
      "Processing:  rvlcdip_test_small/images/memo/0000090729.tif\n",
      "tensor([[ 7.8463e-04, -3.8111e-04, -1.1622e-03, -1.2708e-05, -8.3698e-05,\n",
      "          9.4092e-04, -1.1483e-04, -1.1187e-03,  5.5733e-04, -1.0718e-03,\n",
      "         -6.1258e-04, -1.6633e-04,  6.4932e-04, -2.2683e-04,  1.1428e-03,\n",
      "          3.7943e-04]])\n",
      "   Class: resume\n",
      "   Score: 0.0011428254656493664\n",
      "\n",
      "Processing:  rvlcdip_test_small/images/memo/0000004552.tif\n",
      "tensor([[ 8.0906e-04, -3.7375e-04, -1.1648e-03,  1.2264e-05, -9.6304e-05,\n",
      "          9.2681e-04, -1.3016e-04, -1.0964e-03,  5.4925e-04, -1.0768e-03,\n",
      "         -6.3072e-04, -1.6906e-04,  6.4710e-04, -2.4056e-04,  1.1400e-03,\n",
      "          3.7555e-04]])\n",
      "   Class: resume\n",
      "   Score: 0.0011399956420063972\n",
      "\n",
      "Processing:  rvlcdip_test_small/images/memo/0000047075.tif\n",
      "tensor([[ 7.8806e-04, -3.8402e-04, -1.1425e-03, -4.2627e-06, -7.5598e-05,\n",
      "          9.4132e-04, -1.1332e-04, -1.1307e-03,  5.4420e-04, -1.0555e-03,\n",
      "         -6.0489e-04, -1.4233e-04,  6.5021e-04, -2.3504e-04,  1.1298e-03,\n",
      "          3.7529e-04]])\n",
      "   Class: resume\n",
      "   Score: 0.0011297655291855335\n",
      "\n",
      "Processing:  rvlcdip_test_small/images/memo/0000156419.tif\n",
      "tensor([[ 7.8348e-04, -3.7945e-04, -1.2019e-03,  1.8290e-05, -7.6787e-05,\n",
      "          9.3455e-04, -1.5176e-04, -1.0939e-03,  5.7159e-04, -1.1038e-03,\n",
      "         -6.7527e-04, -1.8888e-04,  6.5951e-04, -2.1889e-04,  1.1303e-03,\n",
      "          3.8508e-04]])\n",
      "   Class: resume\n",
      "   Score: 0.0011303130304440856\n",
      "\n",
      "Processing:  rvlcdip_test_small/images/memo/0000021377.tif\n",
      "tensor([[ 7.6483e-04, -3.9579e-04, -1.1431e-03,  2.2623e-05, -5.4266e-05,\n",
      "          9.3656e-04, -1.1542e-04, -1.1161e-03,  5.6506e-04, -1.0954e-03,\n",
      "         -6.3014e-04, -1.7084e-04,  6.6631e-04, -2.0944e-04,  1.1409e-03,\n",
      "          4.1015e-04]])\n",
      "   Class: resume\n",
      "   Score: 0.001140892505645752\n",
      "\n",
      "Processing:  rvlcdip_test_small/images/memo/0000200170.tif\n",
      "tensor([[ 8.0433e-04, -3.1966e-04, -1.1690e-03, -2.9824e-06, -9.9909e-05,\n",
      "          9.3011e-04, -1.1348e-04, -1.1550e-03,  5.5682e-04, -1.0397e-03,\n",
      "         -5.8277e-04, -1.2954e-04,  6.5016e-04, -2.6212e-04,  1.1720e-03,\n",
      "          3.6993e-04]])\n",
      "   Class: resume\n",
      "   Score: 0.0011719621252268553\n",
      "\n",
      "Processing:  rvlcdip_test_small/images/memo/00073086.tif\n",
      "tensor([[ 7.8173e-04, -3.6924e-04, -1.1641e-03,  6.6453e-06, -7.9282e-05,\n",
      "          9.1885e-04, -1.3318e-04, -1.1016e-03,  5.5854e-04, -1.0912e-03,\n",
      "         -6.2095e-04, -1.6373e-04,  6.5500e-04, -2.3223e-04,  1.1502e-03,\n",
      "          3.8715e-04]])\n",
      "   Class: resume\n",
      "   Score: 0.0011502152774482965\n",
      "\n",
      "Processing:  rvlcdip_test_small/images/memo/0000165280.tif\n",
      "tensor([[ 7.8428e-04, -3.8961e-04, -1.1597e-03,  3.3413e-06, -7.1137e-05,\n",
      "          9.3650e-04, -1.1831e-04, -1.1065e-03,  5.6610e-04, -1.0914e-03,\n",
      "         -6.3494e-04, -1.7307e-04,  6.6834e-04, -2.1469e-04,  1.1537e-03,\n",
      "          3.8448e-04]])\n",
      "   Class: resume\n",
      "   Score: 0.0011537073878571391\n",
      "\n",
      "Processing:  rvlcdip_test_small/images/memo/0000041712.tif\n",
      "tensor([[ 7.9522e-04, -3.5213e-04, -1.1437e-03,  4.7544e-07, -7.6850e-05,\n",
      "          9.4180e-04, -1.0839e-04, -1.1218e-03,  5.3726e-04, -1.1086e-03,\n",
      "         -6.2713e-04, -1.6179e-04,  6.4972e-04, -2.4559e-04,  1.1470e-03,\n",
      "          3.7154e-04]])\n",
      "   Class: resume\n",
      "   Score: 0.0011469651944935322\n",
      "\n",
      "Processing:  rvlcdip_test_small/images/memo/0000035220.tif\n",
      "tensor([[ 7.6948e-04, -3.5489e-04, -1.1436e-03,  9.5461e-07, -8.3288e-05,\n",
      "          9.3282e-04, -1.1783e-04, -1.1194e-03,  5.7032e-04, -1.0860e-03,\n",
      "         -6.5183e-04, -1.6977e-04,  6.8152e-04, -2.1057e-04,  1.1496e-03,\n",
      "          3.9011e-04]])\n",
      "   Class: resume\n",
      "   Score: 0.0011495568323880434\n",
      "\n",
      "Processing:  rvlcdip_test_small/images/memo/0000017140.tif\n",
      "tensor([[ 7.8053e-04, -3.6497e-04, -1.1750e-03,  1.3747e-05, -8.3699e-05,\n",
      "          9.2640e-04, -1.1476e-04, -1.1134e-03,  5.7329e-04, -1.0546e-03,\n",
      "         -6.0778e-04, -1.8332e-04,  6.4175e-04, -2.2477e-04,  1.1582e-03,\n",
      "          3.8271e-04]])\n",
      "   Class: resume\n",
      "   Score: 0.0011582098668441176\n",
      "\n",
      "Processing:  rvlcdip_test_small/images/memo/0000038395.tif\n",
      "tensor([[ 7.8261e-04, -3.7727e-04, -1.1604e-03,  2.5649e-05, -6.3577e-05,\n",
      "          9.1631e-04, -1.3206e-04, -1.1160e-03,  5.5526e-04, -1.0787e-03,\n",
      "         -6.2124e-04, -1.4507e-04,  6.5378e-04, -2.3116e-04,  1.1460e-03,\n",
      "          3.9384e-04]])\n",
      "   Class: resume\n",
      "   Score: 0.0011460238602012396\n",
      "\n",
      "Processing:  rvlcdip_test_small/images/memo/0000009475.tif\n",
      "tensor([[ 7.7417e-04, -3.7833e-04, -1.1823e-03,  4.0351e-06, -7.5160e-05,\n",
      "          9.4384e-04, -1.3863e-04, -1.1084e-03,  5.7297e-04, -1.0783e-03,\n",
      "         -6.3272e-04, -1.8719e-04,  6.5139e-04, -2.1548e-04,  1.1505e-03,\n",
      "          3.9230e-04]])\n",
      "   Class: resume\n",
      "   Score: 0.0011504958383738995\n",
      "\n",
      "Processing:  rvlcdip_test_small/images/memo/0000080997.tif\n",
      "tensor([[ 7.7155e-04, -3.9111e-04, -1.1677e-03, -1.7332e-05, -5.8030e-05,\n",
      "          9.2120e-04, -1.1260e-04, -1.1095e-03,  5.6602e-04, -1.0459e-03,\n",
      "         -6.1640e-04, -1.4034e-04,  6.3855e-04, -1.9430e-04,  1.1439e-03,\n",
      "          3.7288e-04]])\n",
      "   Class: resume\n",
      "   Score: 0.001143879722803831\n",
      "\n",
      "Processing:  rvlcdip_test_small/images/memo/0000041346.tif\n",
      "tensor([[ 7.7348e-04, -3.8703e-04, -1.1496e-03, -3.5446e-06, -7.3075e-05,\n",
      "          9.4421e-04, -1.2034e-04, -1.1154e-03,  5.5310e-04, -1.0837e-03,\n",
      "         -6.2215e-04, -1.6017e-04,  6.4792e-04, -2.2713e-04,  1.1457e-03,\n",
      "          3.7971e-04]])\n",
      "   Class: resume\n",
      "   Score: 0.001145669026300311\n",
      "\n",
      "Processing:  rvlcdip_test_small/images/memo/0000045377.tif\n",
      "tensor([[ 7.8643e-04, -3.7726e-04, -1.1665e-03,  4.1087e-06, -7.6659e-05,\n",
      "          9.3228e-04, -1.2034e-04, -1.1149e-03,  5.5886e-04, -1.0715e-03,\n",
      "         -6.0774e-04, -1.6318e-04,  6.5274e-04, -2.3514e-04,  1.1479e-03,\n",
      "          3.7220e-04]])\n",
      "   Class: resume\n",
      "   Score: 0.0011479377280920744\n",
      "\n",
      "Processing:  rvlcdip_test_small/images/memo/0000136986.tif\n",
      "tensor([[ 7.7990e-04, -3.6804e-04, -1.1549e-03,  9.3952e-06, -6.0655e-05,\n",
      "          9.4739e-04, -1.2981e-04, -1.0981e-03,  5.7260e-04, -1.0956e-03,\n",
      "         -6.4187e-04, -1.7565e-04,  6.3987e-04, -2.0582e-04,  1.1507e-03,\n",
      "          3.9487e-04]])\n",
      "   Class: resume\n",
      "   Score: 0.0011507037561386824\n",
      "\n",
      "Processing:  rvlcdip_test_small/images/memo/0000170247.tif\n",
      "tensor([[ 7.7291e-04, -3.7613e-04, -1.1744e-03,  2.9760e-06, -7.9139e-05,\n",
      "          9.1701e-04, -1.2603e-04, -1.1220e-03,  5.7760e-04, -1.0631e-03,\n",
      "         -6.1801e-04, -1.4789e-04,  6.6130e-04, -2.3749e-04,  1.1548e-03,\n",
      "          3.8167e-04]])\n",
      "   Class: resume\n",
      "   Score: 0.0011548444163054228\n",
      "\n",
      "Processing:  rvlcdip_test_small/images/memo/0000061364.tif\n",
      "tensor([[ 7.7893e-04, -3.3062e-04, -1.1724e-03, -2.4925e-05, -5.7315e-05,\n",
      "          9.2291e-04, -1.4333e-04, -1.1370e-03,  5.5398e-04, -1.0149e-03,\n",
      "         -6.0955e-04, -1.6492e-04,  6.4823e-04, -2.6070e-04,  1.1624e-03,\n",
      "          3.8338e-04]])\n",
      "   Class: resume\n",
      "   Score: 0.0011623525060713291\n",
      "\n",
      "Processing:  rvlcdip_test_small/images/memo/0000066686.tif\n",
      "tensor([[ 7.7354e-04, -4.0173e-04, -1.1476e-03,  9.5655e-06, -4.5643e-05,\n",
      "          9.1110e-04, -1.2205e-04, -1.1223e-03,  5.6089e-04, -1.0771e-03,\n",
      "         -6.4524e-04, -1.1810e-04,  6.6653e-04, -2.1541e-04,  1.1417e-03,\n",
      "          3.8235e-04]])\n",
      "   Class: resume\n",
      "   Score: 0.0011416675988584757\n",
      "\n",
      "Processing:  rvlcdip_test_small/images/memo/0000137405.tif\n",
      "tensor([[ 7.8364e-04, -3.7969e-04, -1.1626e-03,  4.2226e-06, -7.6820e-05,\n",
      "          9.3807e-04, -1.1998e-04, -1.1095e-03,  5.6202e-04, -1.0908e-03,\n",
      "         -6.2560e-04, -1.7382e-04,  6.6297e-04, -2.1974e-04,  1.1458e-03,\n",
      "          3.8319e-04]])\n",
      "   Class: resume\n",
      "   Score: 0.0011457831133157015\n",
      "\n",
      "Processing:  rvlcdip_test_small/images/memo/0000078104.tif\n",
      "tensor([[ 7.9350e-04, -3.7056e-04, -1.1644e-03, -1.2034e-05, -8.7528e-05,\n",
      "          9.4886e-04, -1.0786e-04, -1.1127e-03,  5.6262e-04, -1.0537e-03,\n",
      "         -6.0715e-04, -1.6582e-04,  6.5486e-04, -2.1327e-04,  1.1496e-03,\n",
      "          3.7578e-04]])\n",
      "   Class: resume\n",
      "   Score: 0.001149649964645505\n",
      "\n",
      "Processing:  rvlcdip_test_small/images/memo/0000075334.tif\n",
      "tensor([[ 7.9487e-04, -3.4870e-04, -1.1521e-03,  6.0582e-06, -7.5142e-05,\n",
      "          9.3303e-04, -1.0692e-04, -1.1222e-03,  5.4637e-04, -1.0050e-03,\n",
      "         -5.9672e-04, -1.0409e-04,  6.5768e-04, -2.1907e-04,  1.1600e-03,\n",
      "          3.7424e-04]])\n",
      "   Class: resume\n",
      "   Score: 0.0011599664576351643\n",
      "\n",
      "Processing:  rvlcdip_test_small/images/memo/0000114241.tif\n",
      "tensor([[ 7.7033e-04, -4.0343e-04, -1.1530e-03,  7.1467e-06, -7.5598e-05,\n",
      "          9.2281e-04, -1.0500e-04, -1.1086e-03,  5.5278e-04, -1.0827e-03,\n",
      "         -6.2070e-04, -1.4260e-04,  6.2931e-04, -2.0220e-04,  1.1239e-03,\n",
      "          3.7081e-04]])\n",
      "   Class: resume\n",
      "   Score: 0.0011239285813644528\n",
      "\n",
      "Processing:  rvlcdip_test_small/images/memo/0000109945.tif\n",
      "tensor([[ 7.5771e-04, -3.9444e-04, -1.1631e-03,  1.0346e-05, -5.8475e-05,\n",
      "          8.9868e-04, -1.3809e-04, -1.1169e-03,  5.6788e-04, -1.0719e-03,\n",
      "         -6.1721e-04, -1.4177e-04,  6.1853e-04, -2.2733e-04,  1.1102e-03,\n",
      "          3.7423e-04]])\n",
      "   Class: resume\n",
      "   Score: 0.0011102324351668358\n",
      "\n",
      "Processing:  rvlcdip_test_small/images/memo/0000170942.tif\n",
      "tensor([[ 7.6615e-04, -3.7189e-04, -1.1730e-03,  5.6798e-06, -8.3372e-05,\n",
      "          9.2549e-04, -1.2853e-04, -1.1078e-03,  5.8141e-04, -1.0537e-03,\n",
      "         -6.0614e-04, -1.6238e-04,  6.5372e-04, -2.2702e-04,  1.1490e-03,\n",
      "          3.7754e-04]])\n",
      "   Class: resume\n",
      "   Score: 0.0011489790631458163\n",
      "\n",
      "Processing:  rvlcdip_test_small/images/memo/00005675.tif\n",
      "tensor([[ 7.8439e-04, -3.9467e-04, -1.1606e-03,  6.8502e-06, -8.2787e-05,\n",
      "          9.3162e-04, -1.1883e-04, -1.1111e-03,  5.6675e-04, -1.0907e-03,\n",
      "         -6.3059e-04, -1.7323e-04,  6.5965e-04, -2.2889e-04,  1.1526e-03,\n",
      "          3.7574e-04]])\n",
      "   Class: resume\n",
      "   Score: 0.0011525759473443031\n",
      "\n",
      "Processing:  rvlcdip_test_small/images/memo/0000014916.tif\n",
      "tensor([[ 7.7930e-04, -3.8397e-04, -1.1637e-03, -1.4861e-05, -9.5929e-05,\n",
      "          9.6276e-04, -1.0445e-04, -1.1054e-03,  5.6798e-04, -1.0863e-03,\n",
      "         -6.3207e-04, -2.0216e-04,  6.8818e-04, -2.2798e-04,  1.1405e-03,\n",
      "          3.7677e-04]])\n",
      "   Class: resume\n",
      "   Score: 0.001140464679338038\n",
      "\n",
      "Processing:  rvlcdip_test_small/images/memo/0000129242.tif\n",
      "tensor([[ 7.8019e-04, -3.8815e-04, -1.1677e-03, -1.1467e-05, -7.8034e-05,\n",
      "          9.3505e-04, -1.2087e-04, -1.1082e-03,  5.6609e-04, -1.0856e-03,\n",
      "         -6.1644e-04, -1.6968e-04,  6.6695e-04, -2.2002e-04,  1.1466e-03,\n",
      "          3.8010e-04]])\n",
      "   Class: resume\n",
      "   Score: 0.0011465932475402951\n",
      "\n",
      "Processing:  rvlcdip_test_small/images/memo/0000103791.tif\n",
      "tensor([[ 7.6608e-04, -3.9060e-04, -1.1706e-03, -1.7205e-06, -8.4540e-05,\n",
      "          9.4281e-04, -1.1843e-04, -1.1142e-03,  5.9277e-04, -1.0676e-03,\n",
      "         -6.1030e-04, -1.6318e-04,  6.6433e-04, -2.1451e-04,  1.1522e-03,\n",
      "          3.7476e-04]])\n",
      "   Class: resume\n",
      "   Score: 0.0011522236745804548\n",
      "\n",
      "Processing:  rvlcdip_test_small/images/memo/0000001902.tif\n",
      "tensor([[ 7.6591e-04, -3.8897e-04, -1.1737e-03, -4.4605e-06, -7.6011e-05,\n",
      "          9.3179e-04, -1.2307e-04, -1.1088e-03,  5.7852e-04, -1.0659e-03,\n",
      "         -6.2527e-04, -1.5988e-04,  6.5268e-04, -2.1736e-04,  1.1467e-03,\n",
      "          3.8847e-04]])\n",
      "   Class: resume\n",
      "   Score: 0.0011466515716165304\n",
      "\n",
      "Processing:  rvlcdip_test_small/images/memo/0000089061.tif\n",
      "tensor([[ 7.9458e-04, -3.9491e-04, -1.1676e-03, -1.2615e-05, -7.0902e-05,\n",
      "          9.3698e-04, -1.2464e-04, -1.1153e-03,  5.6890e-04, -1.1094e-03,\n",
      "         -6.2894e-04, -1.7220e-04,  6.7796e-04, -2.3684e-04,  1.1536e-03,\n",
      "          3.8378e-04]])\n",
      "   Class: resume\n",
      "   Score: 0.0011536331148818135\n",
      "\n",
      "Processing:  rvlcdip_test_small/images/memo/0000021894.tif\n",
      "tensor([[ 7.8514e-04, -3.7875e-04, -1.1691e-03, -2.0496e-06, -8.5165e-05,\n",
      "          9.3857e-04, -1.2301e-04, -1.1072e-03,  5.6455e-04, -1.0809e-03,\n",
      "         -6.1953e-04, -1.7257e-04,  6.6841e-04, -2.3331e-04,  1.1486e-03,\n",
      "          3.8336e-04]])\n",
      "   Class: resume\n",
      "   Score: 0.0011486420407891273\n",
      "\n",
      "Processing:  rvlcdip_test_small/images/memo/0000109511.tif\n",
      "tensor([[ 7.7191e-04, -4.0111e-04, -1.1732e-03,  2.1851e-05, -7.9964e-05,\n",
      "          9.1900e-04, -1.2435e-04, -1.1009e-03,  5.6481e-04, -1.0837e-03,\n",
      "         -6.2031e-04, -1.7186e-04,  6.4436e-04, -2.1715e-04,  1.1398e-03,\n",
      "          3.6606e-04]])\n",
      "   Class: resume\n",
      "   Score: 0.0011398487258702517\n",
      "\n",
      "Processing:  rvlcdip_test_small/images/memo/0000087992.tif\n",
      "tensor([[ 7.9044e-04, -3.3537e-04, -1.1448e-03, -2.6297e-05, -7.3620e-05,\n",
      "          9.2239e-04, -9.7787e-05, -1.1287e-03,  5.2436e-04, -9.8194e-04,\n",
      "         -5.5874e-04, -1.2279e-04,  6.2449e-04, -2.0382e-04,  1.1556e-03,\n",
      "          3.9257e-04]])\n",
      "   Class: resume\n",
      "   Score: 0.0011555531527847052\n",
      "\n",
      "Processing:  rvlcdip_test_small/images/memo/0000001053.tif\n",
      "tensor([[ 7.8694e-04, -3.6898e-04, -1.1566e-03, -1.1909e-05, -8.4924e-05,\n",
      "          9.4171e-04, -1.1270e-04, -1.1251e-03,  5.5975e-04, -1.0696e-03,\n",
      "         -6.1420e-04, -1.7328e-04,  6.5327e-04, -2.0989e-04,  1.1481e-03,\n",
      "          3.8146e-04]])\n",
      "   Class: resume\n",
      "   Score: 0.0011481144465506077\n",
      "\n",
      "Processing:  rvlcdip_test_small/images/memo/0000206671.tif\n",
      "tensor([[ 7.7763e-04, -4.0350e-04, -1.1591e-03, -2.0035e-05, -7.7416e-05,\n",
      "          9.4332e-04, -1.0674e-04, -1.1128e-03,  5.6688e-04, -1.0564e-03,\n",
      "         -6.2462e-04, -1.5947e-04,  6.3626e-04, -2.1096e-04,  1.1233e-03,\n",
      "          3.8194e-04]])\n",
      "   Class: resume\n",
      "   Score: 0.0011232835240662098\n",
      "\n",
      "Processing:  rvlcdip_test_small/images/memo/0000146473.tif\n",
      "tensor([[ 7.8271e-04, -3.3684e-04, -1.1678e-03,  1.1510e-05, -1.0823e-04,\n",
      "          9.2634e-04, -1.3975e-04, -1.1222e-03,  5.5533e-04, -1.0682e-03,\n",
      "         -6.0287e-04, -1.8400e-04,  6.3635e-04, -2.4869e-04,  1.1427e-03,\n",
      "          3.5537e-04]])\n",
      "   Class: resume\n",
      "   Score: 0.0011427467688918114\n",
      "\n",
      "Processing:  rvlcdip_test_small/images/memo/0000201554.tif\n",
      "tensor([[ 7.7563e-04, -3.8611e-04, -1.1715e-03,  1.2890e-05, -6.5367e-05,\n",
      "          8.9850e-04, -1.3424e-04, -1.1313e-03,  5.7078e-04, -1.0529e-03,\n",
      "         -5.9702e-04, -1.5142e-04,  6.3100e-04, -2.3613e-04,  1.1346e-03,\n",
      "          3.8493e-04]])\n",
      "   Class: resume\n",
      "   Score: 0.0011345739476382732\n",
      "\n",
      "Processing:  rvlcdip_test_small/images/memo/0000021079.tif\n",
      "tensor([[ 7.6905e-04, -3.7641e-04, -1.1489e-03, -8.0765e-06, -6.1202e-05,\n",
      "          9.2772e-04, -1.1132e-04, -1.1051e-03,  5.6137e-04, -1.0670e-03,\n",
      "         -6.1047e-04, -1.7327e-04,  6.4492e-04, -2.0871e-04,  1.1346e-03,\n",
      "          3.9491e-04]])\n",
      "   Class: resume\n",
      "   Score: 0.0011346268001943827\n",
      "\n",
      "Processing:  rvlcdip_test_small/images/memo/00040465.tif\n",
      "tensor([[ 7.6986e-04, -3.9011e-04, -1.1791e-03, -6.9810e-06, -7.0200e-05,\n",
      "          9.2992e-04, -1.3267e-04, -1.1056e-03,  5.7411e-04, -1.0727e-03,\n",
      "         -6.2588e-04, -1.7494e-04,  6.6137e-04, -2.1550e-04,  1.1479e-03,\n",
      "          3.9352e-04]])\n",
      "   Class: resume\n",
      "   Score: 0.00114791514351964\n",
      "\n",
      "Processing:  rvlcdip_test_small/images/memo/0000137884.tif\n",
      "tensor([[ 8.1931e-04, -3.7670e-04, -1.1666e-03,  1.4673e-05, -1.1230e-04,\n",
      "          9.3252e-04, -1.3067e-04, -1.1051e-03,  5.6328e-04, -1.1043e-03,\n",
      "         -6.1767e-04, -1.8227e-04,  6.7553e-04, -2.4735e-04,  1.1337e-03,\n",
      "          3.4468e-04]])\n",
      "   Class: resume\n",
      "   Score: 0.0011337256291881204\n",
      "\n",
      "Processing:  rvlcdip_test_small/images/memo/0000020623.tif\n",
      "tensor([[ 7.7311e-04, -3.8512e-04, -1.1421e-03,  4.1485e-06, -5.1739e-05,\n",
      "          9.1357e-04, -1.1372e-04, -1.1070e-03,  5.6107e-04, -1.0672e-03,\n",
      "         -6.2377e-04, -1.5180e-04,  6.3860e-04, -2.0061e-04,  1.1438e-03,\n",
      "          3.9957e-04]])\n",
      "   Class: resume\n",
      "   Score: 0.0011438052169978619\n",
      "\n",
      "Processing:  rvlcdip_test_small/images/memo/0000065586.tif\n",
      "tensor([[ 7.8391e-04, -3.7645e-04, -1.1610e-03,  1.6770e-06, -8.2381e-05,\n",
      "          9.3163e-04, -1.1926e-04, -1.1137e-03,  5.5703e-04, -1.0736e-03,\n",
      "         -6.1140e-04, -1.6875e-04,  6.5835e-04, -2.2495e-04,  1.1479e-03,\n",
      "          3.8166e-04]])\n",
      "   Class: resume\n",
      "   Score: 0.001147882896475494\n",
      "\n",
      "Processing:  rvlcdip_test_small/images/memo/0000047354.tif\n",
      "tensor([[ 7.7901e-04, -3.6712e-04, -1.1379e-03,  2.0253e-07, -5.6583e-05,\n",
      "          9.2480e-04, -1.1087e-04, -1.1092e-03,  5.5574e-04, -1.0671e-03,\n",
      "         -6.1197e-04, -1.6167e-04,  6.4295e-04, -2.1015e-04,  1.1380e-03,\n",
      "          3.8522e-04]])\n",
      "   Class: resume\n",
      "   Score: 0.0011379617499187589\n",
      "\n",
      "Processing:  rvlcdip_test_small/images/memo/0000146862.tif\n",
      "tensor([[ 7.5881e-04, -3.8267e-04, -1.1714e-03,  1.0525e-05, -8.4270e-05,\n",
      "          9.3931e-04, -1.2193e-04, -1.1018e-03,  5.8670e-04, -1.0731e-03,\n",
      "         -6.0857e-04, -1.6953e-04,  6.5464e-04, -2.1385e-04,  1.1475e-03,\n",
      "          3.8051e-04]])\n",
      "   Class: resume\n",
      "   Score: 0.0011475333012640476\n",
      "\n",
      "Processing:  rvlcdip_test_small/images/memo/0000092700.tif\n",
      "tensor([[ 7.8944e-04, -3.7205e-04, -1.1649e-03, -1.0923e-05, -9.8794e-05,\n",
      "          9.1954e-04, -1.3487e-04, -1.1079e-03,  5.6862e-04, -1.0885e-03,\n",
      "         -6.0179e-04, -1.8412e-04,  6.6018e-04, -2.3478e-04,  1.1192e-03,\n",
      "          3.7933e-04]])\n",
      "   Class: resume\n",
      "   Score: 0.001119216438382864\n",
      "\n",
      "Processing:  rvlcdip_test_small/images/memo/0000048196.tif\n",
      "tensor([[ 7.9368e-04, -3.5856e-04, -1.1451e-03,  3.3737e-05, -7.0753e-05,\n",
      "          8.7824e-04, -1.2550e-04, -1.1213e-03,  5.4288e-04, -1.0234e-03,\n",
      "         -6.0286e-04, -1.2074e-04,  6.4053e-04, -2.2239e-04,  1.1197e-03,\n",
      "          3.8681e-04]])\n",
      "   Class: resume\n",
      "   Score: 0.0011197240091860294\n",
      "\n",
      "Processing:  rvlcdip_test_small/images/memo/0000050700.tif\n",
      "tensor([[ 7.7872e-04, -3.7467e-04, -1.1580e-03,  2.5657e-05, -7.9701e-05,\n",
      "          9.2739e-04, -1.3360e-04, -1.1024e-03,  5.5591e-04, -1.0942e-03,\n",
      "         -6.1862e-04, -1.7688e-04,  6.5267e-04, -2.2556e-04,  1.1513e-03,\n",
      "          3.8215e-04]])\n",
      "   Class: resume\n",
      "   Score: 0.00115132424980402\n",
      "\n",
      "Processing:  rvlcdip_test_small/images/memo/0000018957.tif\n",
      "tensor([[ 7.8154e-04, -3.6483e-04, -1.1695e-03, -1.3969e-05, -9.2063e-05,\n",
      "          9.3492e-04, -1.1165e-04, -1.1185e-03,  5.6761e-04, -1.0859e-03,\n",
      "         -6.1632e-04, -1.9300e-04,  6.4587e-04, -2.3391e-04,  1.1403e-03,\n",
      "          3.7925e-04]])\n",
      "   Class: resume\n",
      "   Score: 0.0011402920354157686\n",
      "\n",
      "Processing:  rvlcdip_test_small/images/memo/0000051340.tif\n",
      "tensor([[ 7.9745e-04, -4.1492e-04, -1.1560e-03,  1.8204e-05, -6.4929e-05,\n",
      "          9.0697e-04, -1.3291e-04, -1.1084e-03,  5.4744e-04, -1.1055e-03,\n",
      "         -6.3529e-04, -1.4149e-04,  6.4521e-04, -2.1237e-04,  1.1468e-03,\n",
      "          3.7454e-04]])\n",
      "   Class: resume\n",
      "   Score: 0.0011467739241197705\n",
      "\n",
      "Processing:  rvlcdip_test_small/images/memo/0000018970.tif\n",
      "tensor([[ 7.7058e-04, -3.8163e-04, -1.1685e-03, -7.7009e-06, -8.9701e-05,\n",
      "          9.3557e-04, -1.2186e-04, -1.1063e-03,  5.6240e-04, -1.0675e-03,\n",
      "         -6.0497e-04, -1.8453e-04,  6.3595e-04, -2.2482e-04,  1.1386e-03,\n",
      "          3.7274e-04]])\n",
      "   Class: resume\n",
      "   Score: 0.0011385988909751177\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "\n",
    "# Define the path to the directory containing the images\n",
    "image_dir = \"rvlcdip_test_small/images/memo/\"\n",
    "\n",
    "# Iterate over the images in the directory\n",
    "for image_file in os.listdir(image_dir):\n",
    "  # Construct the full path to the image\n",
    "  image_path = os.path.join(image_dir, image_file)\n",
    "  print(\"\\nProcessing: \", image_dir+image_file)\n",
    "  pred_class, score = dit_classifier_torch_inference(image_path, model)\n",
    "\n",
    "  print(\"   Class:\", pred_class)\n",
    "  print(\"   Score:\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RoPAAO6WFg-U"
   },
   "source": [
    "##3.2. Conversion to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5OJ33M47MJxo"
   },
   "outputs": [],
   "source": [
    "dummy_input = torch.randn(1, 3, 224, 224)\n",
    "\n",
    "onnx_path = \"/content/model.onnx\"\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummy_input,\n",
    "    onnx_path,\n",
    "    opset_version=9,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(\"Model exported to:\", onnx_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uskPsg8xROO1"
   },
   "source": [
    "##3.3. ONNX Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h0Vx0Cs1RTH3"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "1DhejbusPxIF"
   ],
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
