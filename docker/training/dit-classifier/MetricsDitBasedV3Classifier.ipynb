{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "736581f2-9190-40e7-b078-08f4edc96226",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Metrics on RVL CDIP for V3 dit based classifier\n",
    "This sample is prepared for running in Databrics, however adapting this to other environments "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c685d8e-f2f5-4bb2-9059-bac18e96ea83",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Disable logging\n",
    "This step is required in some Databricks environments to remove some undesired logging messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6251ecbb-6252-454e-89dd-7e560ca7f0a2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import logging\n",
    "logging.getLogger(\"py4j\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ac1fc0a9-05a2-4e46-97fe-39d71d80ff2f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Mount the S3 bucket\n",
    "We will use DBFS to mount an S3 folder containing the RvlCdip test dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a42c7741-5b6a-49bd-980b-f580a46c7e56",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">/mnt/s3_dev has been unmounted.\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">/mnt/s3_dev has been unmounted.\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "access_key = \"AKIAXXXXXXXXXXXXXXXXXXXX\"\n",
    "secret_key = \"XYZXYZXYZXYZXYZXYZXYZXYZXYZXYZXYZXYZXYZ\"\n",
    "encoded_secret_key = secret_key.replace(\"/\", \"%2F\")\n",
    "aws_bucket_name = \"dev.johnsnowlabs.com\"\n",
    "mount_name = \"s3_dev\"\n",
    "\n",
    "try:\n",
    "  dbutils.fs.mount(\"s3a://%s:%s@%s\" % (access_key, encoded_secret_key, aws_bucket_name), \"/mnt/%s\" % mount_name)\n",
    "except:\n",
    "  dbutils.fs.unmount(\"/mnt/%s\" % mount_name)\n",
    "  dbutils.fs.mount(\"s3a://%s:%s@%s\" % (access_key, encoded_secret_key, aws_bucket_name), \"/mnt/%s\" % mount_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2245fe4c-35fe-4f4d-97ff-df02fc27ca7f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's take a look at the contents of the dataset folder. Each document image is placed within a specific folder reflecting the name of the class the document belongs to.</br>\n",
    "The RvlCdipReader.readTestDataset() function will lift the directory structure we just described and create an 'act_label' column in a PySpark Dataframe.</br>\n",
    "More on this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2726464a-ddc8-4b3d-945f-330d979a4c94",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">advertisement  form\t    memo\t   resume\r\n",
       "budget\t       handwritten  news_article   scientific_publication\r\n",
       "email\t       invoice\t    presentation   scientific_report\r\n",
       "file_folder    letter\t    questionnaire  specification\r\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">advertisement  form\t    memo\t   resume\r\nbudget\t       handwritten  news_article   scientific_publication\r\nemail\t       invoice\t    presentation   scientific_report\r\nfile_folder    letter\t    questionnaire  specification\r\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "!ls /dbfs/mnt/s3_dev/ocr/datasets/RVL-CDIP/test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fdaf34cd-34fa-409a-b5ee-56cb80ebbb3a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Set up license\n",
    "If you are in Databricks, and haven't used any other mechanism for setting the license, paste your license key in this cell and update the license file accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3c6b4b5-641a-42db-ab33-f37d4449526a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%bash\n",
    "rm /dbfs/FileStore/johnsnowlabs/license.key\n",
    "echo \"XYZXYZXYZXYZXYZXYZXYZXYZXYZXYZXYZXYZXYZXYZXYZXYZXYZXYZXYZXYZXYZXYZXYZXYZ\" >> /dbfs/FileStore/johnsnowlabs/license.key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a0272ec4-b5de-40a9-97ad-607dad7bd3b8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Read the dataset\n",
    "Let's first take a look at the documentation,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b82489fd-9a81-400f-b929-ce5ebd9206e1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function readTestDataset in module sparkocr.transformers.readers.rvlcdip_reader:\n",
      "\n",
      "readTestDataset(self, spark, path, partitions=8, storage_level=StorageLevel(True, False, False, False, 1))\n",
      "    Reads the RVL-CDP train dataset from an external resource.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    spark : :class:`pyspark.sql.SparkSession`\n",
      "        Initiated Spark Session with Spark NLP\n",
      "    path : str\n",
      "        the path where you unzip the files for RvlCdip Test\n",
      "    partitions : sets the minimum number of partitions for the case of lifting multiple files in parallel into a single dataframe. Defaults to 8.\n",
      "    storage_level : sets the persistence level according to PySpark definitions. Defaults to StorageLevel.DISK_ONLY.\n",
      "    \n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    :class:`pyspark.sql.DataFrame`\n",
      "        Spark Dataframe with the data\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sparkocr.transformers.readers.rvlcdip_reader import RvlCdipReader\n",
    "help(RvlCdipReader.readTestDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50b2884c-26e0-436e-8bc2-5ce1566dfd82",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "datasetPath = \"dbfs:/mnt/s3_dev/ocr/datasets/RVL-CDIP/test/\"\n",
    "df = RvlCdipReader().readTestDataset(spark, datasetPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "909f6bc2-6f7c-4c54-a692-ed078ac0334d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's unpack these outputs, we have the path, which is the invdividual file location on disk, we have the content that is the binary information, and the act_label that is the label coming from the dataset.<br>\n",
    "Let's check how many images  we have,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27269251-4d31-4fcf-ad19-760410c0c5a6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[7]: 39947</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[7]: 39947</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "total_images = df.select(\"path\").count()\n",
    "total_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "85f92db0-2fd9-4f44-a6c2-c332c1470cfd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's take a look at the datasets, the partitioning, and also at some specific records to make sure everything is ok."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "828cb534-ec9d-4a57-9e7e-f677a76f2b95",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[8]: 1266</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[8]: 1266</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "r = df.rdd\n",
    "r.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82a8be2e-f5d8-4f69-ae97-3e0ee7c058e0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.select(\"path\", \"act_label\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "19afc440-0b12-4eed-9790-ea4c2553816c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Define the pipeline\n",
    "Let's define our pipeline using 2 transformers: BinaryToImage and VisualDocumentClassifierV3. </br>\n",
    "The first one is responsible for transforming the binary content into an image structure decoding things like number of channels, resolution, etc. The second one is the model perse, check that the predicted label will go in column 'label'(which is different from dataset labels that will be in 'act_label' column)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16624ca4-ac25-47c2-ad63-1f345bbdb28d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sparkocr.transformers import *\n",
    "from sparkocr.enums import *\n",
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "binary_to_image = BinaryToImage()\\\n",
    "    .setOutputCol(\"image\") \\\n",
    "    .setImageType(ImageType.TYPE_3BYTE_BGR)\n",
    "\n",
    "doc_class = VisualDocumentClassifierV3() \\\n",
    "    .pretrained(\"dit_base_finetuned_rvlcdip_opt\", \"en\", \"clinical/ocr\") \\\n",
    "    .setInputCols([\"image\"]) \\\n",
    "    .setOutputCol(\"label\")\n",
    "\n",
    "# OCR pipeline\n",
    "pipeline = PipelineModel(stages=[\n",
    "    binary_to_image,\n",
    "    doc_class\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8dd43721-c1e6-498b-8339-344590d019d2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "predicted = pipeline.transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df93428a-b46c-4bcb-a79c-92d196289823",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "At this point, 'predicted' holds our results, no computation has happened so far, the dataframe at this point contains the 'recipe' for classifying our dataset, nothing will happen until some action happens in the Dataframe.</br>\n",
    "We will use the write.parquet() action to store everything to disk, in case we want to consume the resuls multiple times, or use it for something else later.</br>\n",
    "Let's go!,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "329b56b7-a6ec-491b-9a8e-1aaca9519a79",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "predicted.select(\"label\", \"act_label\", \"exception\").write.parquet(\"predictions_pipeline.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ac6038e-c16a-4504-986b-af5b49ebd22b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Compute Metrics\n",
    "Here we will compute accuracy, you can try other metrics as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc4186eb-720d-4d0e-abcc-eae5340fa545",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[17]: 0.9155631211355045</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[17]: 0.9155631211355045</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "predicted = spark.read.parquet('dbfs:/predictions_pipeline.parquet')\n",
    "it = predicted.toLocalIterator()\n",
    "\n",
    "\n",
    "total_images = total_images\n",
    "correct = 0\n",
    "empty = 0\n",
    "#for row in tqdm(it, total=total_images):\n",
    "for row in it:\n",
    "  gold = row.asDict()['act_label']\n",
    "  predicted = row.asDict()['label']\n",
    "  if predicted == '':\n",
    "    empty += 1\n",
    "  if gold.replace(\"_\", \"\") == predicted.replace(\" \", \"\"):\n",
    "      correct += 1\n",
    "\n",
    "float(correct)/total_images      "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "MetricsDitBasedV3Classifier",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
