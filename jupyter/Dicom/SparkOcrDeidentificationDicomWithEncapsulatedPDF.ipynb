{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "843283de-d34a-434a-9fae-e6729d3ffd23",
   "metadata": {},
   "source": [
    "# De-identification Dicom documents with encapsulated Pdf document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c95142",
   "metadata": {},
   "source": [
    "## Install spark-ocr python packge\n",
    "Need specify:\n",
    "- license\n",
    "- path to `spark-ocr-assembly-[version].jar` and `spark-nlp-jsl-[version]`\n",
    "- or `secret` for Spark OCR and `nlp_secret` for Spark NLP Internal\n",
    "- `aws_access_key` and `aws_secret_key`for download pretrained models\n",
    "\n",
    "For more details about Dicom de-identification please read:\n",
    "\n",
    " - [DICOM de-identification at scale in Visual NLP — Part 1.](https://medium.com/john-snow-labs/dicom-de-identification-at-scale-in-visual-nlp-part-1-68784177f5f0)\n",
    "\n",
    " - [DICOM de-identification at scale in Visual NLP — Part 2.](https://medium.com/john-snow-labs/dicom-de-identification-at-scale-in-visual-nlp-part-2-361af5e36412)\n",
    "\n",
    " - [DICOM de-identification at scale in Visual NLP — Part 3.](https://medium.com/john-snow-labs/dicom-de-identification-at-scale-in-visual-nlp-part-3-ac750be386cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61b16a4a-138f-4d7d-bb6b-b15a6e5ccb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "license = \"\"\n",
    "secret = \"\"\n",
    "nlp_secret = \"\"\n",
    "aws_access_key = \"\"\n",
    "aws_secret_key = \"\"\n",
    "\n",
    "version = secret.split(\"-\")[0]\n",
    "nlp_internal_version = str(nlp_secret.split(\"-\")[0])\n",
    "spark_ocr_jar_path = \"../../../target/scala-2.12\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e662872-2e3d-4dbf-951b-ae363be3dcf9",
   "metadata": {},
   "source": [
    "## Install requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b674f64-0058-49b1-8f54-04ba92c419de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NBVAL_SKIP\n",
    "%pip install pydicom highdicom\n",
    "%pip install --upgrade spark-nlp-jsl==5.1.1  --extra-index-url https://pypi.johnsnowlabs.com/$nlp_secret\n",
    "%pip install spark-nlp==5.1.1\n",
    "%pip install spark-ocr==$version --extra-index-url=https://pypi.johnsnowlabs.com/$secret --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f3b25b-a5d5-4cba-bcac-3ac8db699eec",
   "metadata": {},
   "source": [
    "## Start Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a1aca55-7435-46d2-844f-780023c35f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 3.5.0\n",
      "Spark NLP version: 5.2.2\n",
      "Spark NLP for Healthcare version: 5.2.1\n",
      "Spark OCR version: 5.2.0\n",
      "\n",
      ":: loading settings :: url = jar:file:/opt/conda/envs/trocrMetrics/lib/python3.9/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/ec2-user/.ivy2/cache\n",
      "The jars for the packages stored in: /home/ec2-user/.ivy2/jars\n",
      "com.johnsnowlabs.nlp#spark-nlp_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-4fb78512-0448-4517-806a-dbf83f6920a4;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.johnsnowlabs.nlp#spark-nlp_2.12;5.2.2 in central\n",
      "\tfound com.typesafe#config;1.4.2 in central\n",
      "\tfound org.rocksdb#rocksdbjni;6.29.5 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.12.500 in central\n",
      "\tfound com.github.universal-automata#liblevenshtein;3.0.0 in central\n",
      "\tfound com.google.protobuf#protobuf-java-util;3.0.0-beta-3 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.0.0-beta-3 in central\n",
      "\tfound com.google.code.gson#gson;2.3 in central\n",
      "\tfound it.unimi.dsi#fastutil;7.0.12 in spark-list\n",
      "\tfound org.projectlombok#lombok;1.16.8 in central\n",
      "\tfound com.google.cloud#google-cloud-storage;2.20.1 in spark-list\n",
      "\tfound com.google.guava#guava;31.1-jre in spark-list\n",
      "\tfound com.google.guava#failureaccess;1.0.1 in central\n",
      "\tfound com.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava in central\n",
      "\tfound com.google.errorprone#error_prone_annotations;2.18.0 in central\n",
      "\tfound com.google.j2objc#j2objc-annotations;1.3 in central\n",
      "\tfound com.google.http-client#google-http-client;1.43.0 in central\n",
      "\tfound io.opencensus#opencensus-contrib-http-util;0.31.1 in central\n",
      "\tfound com.google.http-client#google-http-client-jackson2;1.43.0 in central\n",
      "\tfound com.google.http-client#google-http-client-gson;1.43.0 in central\n",
      "\tfound com.google.api-client#google-api-client;2.2.0 in central\n",
      "\tfound commons-codec#commons-codec;1.15 in central\n",
      "\tfound com.google.oauth-client#google-oauth-client;1.34.1 in central\n",
      "\tfound com.google.http-client#google-http-client-apache-v2;1.43.0 in central\n",
      "\tfound com.google.apis#google-api-services-storage;v1-rev20220705-2.0.0 in central\n",
      "\tfound com.google.code.gson#gson;2.10.1 in central\n",
      "\tfound com.google.cloud#google-cloud-core;2.12.0 in central\n",
      "\tfound io.grpc#grpc-context;1.53.0 in central\n",
      "\tfound com.google.auto.value#auto-value-annotations;1.10.1 in central\n",
      "\tfound com.google.auto.value#auto-value;1.10.1 in central\n",
      "\tfound javax.annotation#javax.annotation-api;1.3.2 in central\n",
      "\tfound commons-logging#commons-logging;1.2 in central\n",
      "\tfound com.google.cloud#google-cloud-core-http;2.12.0 in central\n",
      "\tfound com.google.http-client#google-http-client-appengine;1.43.0 in central\n",
      "\tfound com.google.api#gax-httpjson;0.108.2 in central\n",
      "\tfound com.google.cloud#google-cloud-core-grpc;2.12.0 in central\n",
      "\tfound io.grpc#grpc-alts;1.53.0 in central\n",
      "\tfound io.grpc#grpc-grpclb;1.53.0 in central\n",
      "\tfound org.conscrypt#conscrypt-openjdk-uber;2.5.2 in central\n",
      "\tfound io.grpc#grpc-auth;1.53.0 in central\n",
      "\tfound io.grpc#grpc-protobuf;1.53.0 in central\n",
      "\tfound io.grpc#grpc-protobuf-lite;1.53.0 in central\n",
      "\tfound io.grpc#grpc-core;1.53.0 in central\n",
      "\tfound com.google.api#gax;2.23.2 in central\n",
      "\tfound com.google.api#gax-grpc;2.23.2 in central\n",
      "\tfound com.google.auth#google-auth-library-credentials;1.16.0 in central\n",
      "\tfound com.google.auth#google-auth-library-oauth2-http;1.16.0 in central\n",
      "\tfound com.google.api#api-common;2.6.2 in central\n",
      "\tfound io.opencensus#opencensus-api;0.31.1 in central\n",
      "\tfound com.google.api.grpc#proto-google-iam-v1;1.9.2 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.21.12 in central\n",
      "\tfound com.google.protobuf#protobuf-java-util;3.21.12 in central\n",
      "\tfound com.google.api.grpc#proto-google-common-protos;2.14.2 in central\n",
      "\tfound org.threeten#threetenbp;1.6.5 in central\n",
      "\tfound com.google.api.grpc#proto-google-cloud-storage-v2;2.20.1-alpha in central\n",
      "\tfound com.google.api.grpc#grpc-google-cloud-storage-v2;2.20.1-alpha in central\n",
      "\tfound com.google.api.grpc#gapic-google-cloud-storage-v2;2.20.1-alpha in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-core;2.14.2 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
      "\tfound io.grpc#grpc-api;1.53.0 in central\n",
      "\tfound io.grpc#grpc-stub;1.53.0 in central\n",
      "\tfound org.checkerframework#checker-qual;3.31.0 in central\n",
      "\tfound io.perfmark#perfmark-api;0.26.0 in central\n",
      "\tfound com.google.android#annotations;4.1.1.4 in central\n",
      "\tfound org.codehaus.mojo#animal-sniffer-annotations;1.22 in central\n",
      "\tfound io.opencensus#opencensus-proto;0.2.0 in central\n",
      "\tfound io.grpc#grpc-services;1.53.0 in central\n",
      "\tfound com.google.re2j#re2j;1.6 in central\n",
      "\tfound io.grpc#grpc-netty-shaded;1.53.0 in central\n",
      "\tfound io.grpc#grpc-googleapis;1.53.0 in central\n",
      "\tfound io.grpc#grpc-xds;1.53.0 in central\n",
      "\tfound com.navigamez#greex;1.0 in central\n",
      "\tfound dk.brics.automaton#automaton;1.11-8 in central\n",
      "\tfound com.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.4.4 in central\n",
      "\tfound com.microsoft.onnxruntime#onnxruntime;1.16.3 in central\n",
      ":: resolution report :: resolve 854ms :: artifacts dl 31ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.500 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-core;2.14.2 from central in [default]\n",
      "\tcom.github.universal-automata#liblevenshtein;3.0.0 from central in [default]\n",
      "\tcom.google.android#annotations;4.1.1.4 from central in [default]\n",
      "\tcom.google.api#api-common;2.6.2 from central in [default]\n",
      "\tcom.google.api#gax;2.23.2 from central in [default]\n",
      "\tcom.google.api#gax-grpc;2.23.2 from central in [default]\n",
      "\tcom.google.api#gax-httpjson;0.108.2 from central in [default]\n",
      "\tcom.google.api-client#google-api-client;2.2.0 from central in [default]\n",
      "\tcom.google.api.grpc#gapic-google-cloud-storage-v2;2.20.1-alpha from central in [default]\n",
      "\tcom.google.api.grpc#grpc-google-cloud-storage-v2;2.20.1-alpha from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-cloud-storage-v2;2.20.1-alpha from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-common-protos;2.14.2 from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-iam-v1;1.9.2 from central in [default]\n",
      "\tcom.google.apis#google-api-services-storage;v1-rev20220705-2.0.0 from central in [default]\n",
      "\tcom.google.auth#google-auth-library-credentials;1.16.0 from central in [default]\n",
      "\tcom.google.auth#google-auth-library-oauth2-http;1.16.0 from central in [default]\n",
      "\tcom.google.auto.value#auto-value;1.10.1 from central in [default]\n",
      "\tcom.google.auto.value#auto-value-annotations;1.10.1 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core;2.12.0 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core-grpc;2.12.0 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core-http;2.12.0 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-storage;2.20.1 from spark-list in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.10.1 from central in [default]\n",
      "\tcom.google.errorprone#error_prone_annotations;2.18.0 from central in [default]\n",
      "\tcom.google.guava#failureaccess;1.0.1 from central in [default]\n",
      "\tcom.google.guava#guava;31.1-jre from spark-list in [default]\n",
      "\tcom.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava from central in [default]\n",
      "\tcom.google.http-client#google-http-client;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-apache-v2;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-appengine;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-gson;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-jackson2;1.43.0 from central in [default]\n",
      "\tcom.google.j2objc#j2objc-annotations;1.3 from central in [default]\n",
      "\tcom.google.oauth-client#google-oauth-client;1.34.1 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.21.12 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java-util;3.21.12 from central in [default]\n",
      "\tcom.google.re2j#re2j;1.6 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#spark-nlp_2.12;5.2.2 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.4.4 from central in [default]\n",
      "\tcom.microsoft.onnxruntime#onnxruntime;1.16.3 from central in [default]\n",
      "\tcom.navigamez#greex;1.0 from central in [default]\n",
      "\tcom.typesafe#config;1.4.2 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.15 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.2 from central in [default]\n",
      "\tdk.brics.automaton#automaton;1.11-8 from central in [default]\n",
      "\tio.grpc#grpc-alts;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-api;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-auth;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-context;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-core;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-googleapis;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-grpclb;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-netty-shaded;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-protobuf;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-protobuf-lite;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-services;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-stub;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-xds;1.53.0 from central in [default]\n",
      "\tio.opencensus#opencensus-api;0.31.1 from central in [default]\n",
      "\tio.opencensus#opencensus-contrib-http-util;0.31.1 from central in [default]\n",
      "\tio.opencensus#opencensus-proto;0.2.0 from central in [default]\n",
      "\tio.perfmark#perfmark-api;0.26.0 from central in [default]\n",
      "\tit.unimi.dsi#fastutil;7.0.12 from spark-list in [default]\n",
      "\tjavax.annotation#javax.annotation-api;1.3.2 from central in [default]\n",
      "\torg.checkerframework#checker-qual;3.31.0 from central in [default]\n",
      "\torg.codehaus.mojo#animal-sniffer-annotations;1.22 from central in [default]\n",
      "\torg.conscrypt#conscrypt-openjdk-uber;2.5.2 from central in [default]\n",
      "\torg.projectlombok#lombok;1.16.8 from central in [default]\n",
      "\torg.rocksdb#rocksdbjni;6.29.5 from central in [default]\n",
      "\torg.threeten#threetenbp;1.6.5 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\tcom.google.protobuf#protobuf-java-util;3.0.0-beta-3 by [com.google.protobuf#protobuf-java-util;3.21.12] in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.0.0-beta-3 by [com.google.protobuf#protobuf-java;3.21.12] in [default]\n",
      "\tcom.google.code.gson#gson;2.3 by [com.google.code.gson#gson;2.10.1] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   75  |   0   |   0   |   3   ||   72  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-4fb78512-0448-4517-806a-dbf83f6920a4\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 72 already retrieved (0kB/21ms)\n",
      "24/02/27 02:14:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/02/27 02:14:35 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-31-34-182.us-east-2.compute.internal:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark OCR</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fc049e11e20>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sparkocr import start\n",
    "import os\n",
    "from pyspark import SparkConf\n",
    "\n",
    "if license:\n",
    "    os.environ['JSL_OCR_LICENSE'] = license\n",
    "    os.environ['SPARK_NLP_LICENSE'] = license\n",
    "\n",
    "if aws_access_key:\n",
    "    os.environ['AWS_ACCESS_KEY'] = aws_access_key\n",
    "    os.environ['AWS_SECRET_ACCESS_KEY'] = aws_secret_key\n",
    "\n",
    "\n",
    "spark = start(secret=secret,\n",
    "              nlp_secret=nlp_secret,\n",
    "              jar_path=spark_ocr_jar_path,\n",
    "              nlp_internal=nlp_internal_version\n",
    "             )\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c383814d-51cf-4bcb-b3f8-2c5baf084968",
   "metadata": {},
   "source": [
    "## Import transformers and annotators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb0a451a-6b20-4281-b40a-837048ced908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark NLP version: 5.2.2\n",
      "Spark NLP internal version: 5.2.1\n",
      "Spark OCR version: 5.2.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.base import *\n",
    "import sparknlp_jsl\n",
    "from sparknlp_jsl.annotator import *\n",
    "\n",
    "import sparkocr\n",
    "from sparkocr.transformers import *\n",
    "from sparkocr.utils import *\n",
    "from sparkocr.enums import *\n",
    "from sparkocr.schemas import BinarySchema\n",
    "\n",
    "from pyspark.ml import PipelineModel, Pipeline\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "print(f\"Spark NLP version: {sparknlp.version()}\")\n",
    "print(f\"Spark NLP internal version: {sparknlp_jsl.version()}\")\n",
    "print(f\"Spark OCR version: {sparkocr.version()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe452f1-df0d-489a-865a-85912ef721a2",
   "metadata": {},
   "source": [
    "## Define Spark NLP pipeline for de-identification text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9de8226-5864-48e4-aeca-fc002ba29983",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deidentification_nlp_pipeline(input_column, prefix = \"\", model=\"ner_deid_large\"):\n",
    "    document_assembler = DocumentAssembler() \\\n",
    "        .setInputCol(input_column) \\\n",
    "        .setOutputCol(prefix + \"document_raw\")\n",
    "\n",
    "    cleanUpPatterns = [\"<[^>]*>\", \":\"]\n",
    "    documentNormalizer = DocumentNormalizer() \\\n",
    "      .setInputCols(prefix + \"document_raw\") \\\n",
    "      .setOutputCol(prefix + \"document\") \\\n",
    "      .setAction(\"clean\") \\\n",
    "      .setPatterns(cleanUpPatterns) \\\n",
    "      .setReplacement(\" \") \\\n",
    "      .setPolicy(\"pretty_all\") \n",
    "\n",
    "    # Sentence Detector annotator, processes various sentences per line\n",
    "    sentence_detector = SentenceDetector() \\\n",
    "        .setInputCols([prefix + \"document\"]) \\\n",
    "        .setOutputCol(prefix + \"sentence\")\n",
    "\n",
    "    tokenizer = Tokenizer() \\\n",
    "        .setInputCols([prefix + \"sentence\"]) \\\n",
    "        .setOutputCol(prefix + \"token\")\n",
    "\n",
    "    # Clinical word embeddings\n",
    "    word_embeddings = WordEmbeddingsModel.pretrained(\"embeddings_clinical\", \"en\", \"clinical/models\") \\\n",
    "        .setInputCols([prefix + \"sentence\", prefix + \"token\"]) \\\n",
    "        .setOutputCol(prefix + \"embeddings\") \\\n",
    "        .setEnableInMemoryStorage(True)\n",
    "\n",
    "    clinical_ner = MedicalNerModel.pretrained(model, \"en\", \"clinical/models\") \\\n",
    "        .setInputCols([prefix + \"sentence\", prefix + \"token\", prefix + \"embeddings\"]) \\\n",
    "        .setOutputCol(prefix + \"ner\")\n",
    "\n",
    "    custom_ner_converter = NerConverter() \\\n",
    "        .setInputCols([prefix + \"sentence\", prefix + \"token\", prefix + \"ner\"]) \\\n",
    "        .setOutputCol(prefix + \"ner_chunk\") \\\n",
    "        .setWhiteList(['NAME', 'AGE', 'CONTACT', 'ID',\n",
    "                   'LOCATION', 'PROFESSION', 'PERSON', 'DATE', 'DOCTOR'])\n",
    "\n",
    "    nlp_pipeline = Pipeline(stages=[\n",
    "            document_assembler,\n",
    "            documentNormalizer,\n",
    "            sentence_detector,\n",
    "            tokenizer,\n",
    "            word_embeddings,\n",
    "            clinical_ner,\n",
    "            custom_ner_converter\n",
    "        ])\n",
    "    empty_data = spark.createDataFrame([[\"\"]]).toDF(input_column)\n",
    "    nlp_model = nlp_pipeline.fit(empty_data)\n",
    "    return nlp_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03817e0d-7b17-423b-ba52-1ed625cf4a75",
   "metadata": {},
   "source": [
    "## Define Spark Ocr pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd8baf07-8ced-4671-af56-6e228d0aa022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings_clinical download started this may take some time.\n",
      "Approximate size to download 1.6 GB\n",
      "[OK!]\n",
      "ner_deid_generic_augmented download started this may take some time.\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "# Extract encapsulated Pdf from the Dicom\n",
    "dicom_to_pdf = DicomToPdf() \\\n",
    "    .setInputCols([\"path\"]) \\\n",
    "    .setOutputCol(\"pdf\") \\\n",
    "    .setKeepInput(True)\n",
    "\n",
    "# Convert Pdf to the image\n",
    "pdf_to_image = PdfToImage() \\\n",
    "    .setInputCol(\"pdf\") \\\n",
    "    .setOutputCol(\"image\") \\\n",
    "    .setFallBackCol(\"text_image\")\n",
    "\n",
    "# Recognize text\n",
    "ocr = ImageToText() \\\n",
    "    .setInputCol(\"image\") \\\n",
    "    .setOutputCol(\"text\") \\\n",
    "    .setIgnoreResolution(False) \\\n",
    "    .setPageIteratorLevel(PageIteratorLevel.SYMBOL) \\\n",
    "    .setPageSegMode(PageSegmentationMode.SPARSE_TEXT) \\\n",
    "    .setConfidenceThreshold(70)\n",
    "\n",
    "# Found coordinates of sensitive data\n",
    "position_finder = PositionFinder() \\\n",
    "    .setInputCols(\"ner_chunk\") \\\n",
    "    .setOutputCol(\"regions\") \\\n",
    "    .setPageMatrixCol(\"positions\") \\\n",
    "    .setOcrScaleFactor(1)\n",
    "\n",
    "# Hide sensitive data\n",
    "drawRegions = ImageDrawRegions()  \\\n",
    "    .setInputCol(\"image\")  \\\n",
    "    .setInputRegionsCol(\"regions\")  \\\n",
    "    .setOutputCol(\"image_with_regions\")  \\\n",
    "    .setFilledRect(True) \\\n",
    "    .setRectColor(Color.gray)\n",
    "\n",
    "# Convert image to Pdf\n",
    "image_to_pdf = ImageToPdf() \\\n",
    "    .setInputCol(\"image_with_regions\") \\\n",
    "    .setOutputCol(\"pdf\")\n",
    "\n",
    "# Update Pdf in Dicom\n",
    "dciom_update_pdf = DicomUpdatePdf() \\\n",
    "    .setInputCol(\"path\") \\\n",
    "    .setInputPdfCol(\"pdf\") \\\n",
    "    .setOutputCol(\"dicom\") \\\n",
    "    .setKeepInput(True)\n",
    "\n",
    "# Deidentify metadata in Dicom\n",
    "dicom_deidentifier = DicomMetadataDeidentifier() \\\n",
    "    .setInputCols([\"dicom\"]) \\\n",
    "    .setOutputCol(\"dicom_cleaned\")\n",
    "\n",
    "# OCR pipeline\n",
    "pipeline = PipelineModel(stages=[\n",
    "     dicom_to_pdf,\n",
    "     pdf_to_image,\n",
    "     ocr,\n",
    "     deidentification_nlp_pipeline(input_column=\"text\", prefix=\"\", model=\"ner_deid_generic_augmented\"),\n",
    "     position_finder,\n",
    "     drawRegions,\n",
    "     image_to_pdf,\n",
    "     dciom_update_pdf,\n",
    "     dicom_deidentifier\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0960ab-c815-460a-bd2d-705a91d494e7",
   "metadata": {},
   "source": [
    "## Read dicom files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d53c3eee-8649-44c9-b3b8-cacc3c36447c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dicom_path = './../data/dicom/encapsulated/*.dcm'\n",
    "dicom_df = spark.read.format(\"binaryFile\").load(dicom_path)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f9234c8-f4f0-4fff-8c2d-3b9e300c011f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+------+--------------------+\n",
      "|                path|   modificationTime|length|             content|\n",
      "+--------------------+-------------------+------+--------------------+\n",
      "|file:/home/ec2-us...|2024-02-26 22:43:57|651696|[00 00 00 00 00 0...|\n",
      "+--------------------+-------------------+------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dicom_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed70cd1b-78bf-4bdf-93e5-2e344ab2f03b",
   "metadata": {},
   "source": [
    "## Run pipeline and store resulst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b843cc7d-5f10-4c6d-af2c-6786e297da54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/27 02:16:43 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "Warning: Version of org.bytedeco:leptonica could not be found.    (0 + 16) / 16]\n",
      "Warning: Version of org.bytedeco:tesseract could not be found.    (11 + 5) / 16]\n",
      "24/02/27 02:16:55 ERROR PositionFinder: PositionFinder unmatched:::Annotation(type: chunk, begin: 946, end: 1000, result: Industries Served Computer software, Banking, Insurance), index: 9\n",
      "02:17:00, INFO Run DicomMetadataDeidentifier                        (0 + 1) / 1]\n",
      "/opt/conda/envs/trocrMetrics/lib/python3.9/site-packages/pydicom/dataset.py:594: UserWarning: Invalid value '(-1025, 1)' used with the 'in' operator: must be an element tag as a 2-tuple or int, or an element keyword\n",
      "  warnings.warn(msg)\n",
      "24/02/27 02:17:01 WARN BasicWriteTaskStatsTracker: Expected 1 files, but only saw 0. This could be due to the output format not writing empty files, or files being not immediately visible in the filesystem.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# NBVAL_SKIP\n",
    "output_path = \"./deidentified_pdf/\"\n",
    "\n",
    "def get_name(path, keep_subfolder_level=0):\n",
    "    path = path.split(\"/\")\n",
    "    path[-1] = \".\".join(path[-1].split('.')[:-1])\n",
    "    return \"/\".join(path[-keep_subfolder_level-1:])\n",
    "\n",
    "result = pipeline.transform(dicom_df)\n",
    "result.withColumn(\"fileName\", udf(get_name, StringType())(col(\"path\"))) \\\n",
    "    .write \\\n",
    "    .format(\"binaryFormat\") \\\n",
    "    .option(\"type\", \"dicom\") \\\n",
    "    .option(\"field\", \"dicom_cleaned\") \\\n",
    "    .option(\"prefix\", \"\") \\\n",
    "    .option(\"nameField\", \"fileName\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91ca07a",
   "metadata": {},
   "source": [
    "## Remove results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "458c1876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NBVAL_SKIP\n",
    "%%bash\n",
    "rm -r -f ./deidentified_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6826404-d2fb-418d-961e-f008d9ad6cdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
